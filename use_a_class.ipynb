{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys, file and nav packages:\n",
    "import os\n",
    "import datetime as dt\n",
    "import csv\n",
    "\n",
    "# math packages:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import binom\n",
    "import datetime as dt \n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from scipy.stats import kendalltau, pearsonr, spearmanr\n",
    "import math\n",
    "from scipy.stats import gamma\n",
    "from scipy.stats import beta\n",
    "\n",
    "# charting:\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib import ticker\n",
    "import matplotlib.ticker as mtick\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from IPython.display import display, Markdown, Latex, HTML\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap, Colormap\n",
    "\n",
    "# mapping\n",
    "import geopandas as gpd\n",
    "\n",
    "# home brew utitilties\n",
    "import utilities.utility_functions as ut\n",
    "import utilities.abundance_classes as ac\n",
    "\n",
    "# documenting\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "\n",
    "# returns the p_value for each test\n",
    "def kendall_pval(x,y):\n",
    "    return kendalltau(x,y)[1]\n",
    "\n",
    "def pearsonr_pval(x,y):\n",
    "    return pearsonr(x,y)[1]\n",
    "\n",
    "def spearmanr_pval(x,y):\n",
    "    return spearmanr(x,y)[1]\n",
    "\n",
    "# table kwargs\n",
    "table_k = dict(loc=\"top left\", bbox=(0,0,1,1), colWidths=[.5, .5], cellLoc='center')\n",
    "tablecenter_k = dict(loc=\"top left\", bbox=(0,0,1,1), cellLoc='center')\n",
    "tabtickp_k = dict(axis='both', which='both', bottom=False, top=False, left=False, right=False, labelleft=False, labelbottom=False)\n",
    "\n",
    "# chart kwargs\n",
    "title_k = {'loc':'left', 'pad':14, 'linespacing':1.5, 'fontsize':12}\n",
    "title_k14 = {'loc':'left', 'pad':16, 'linespacing':1.5, 'fontsize':14}\n",
    "xlab_k = {'labelpad':10, 'fontsize':12}\n",
    "ylab_k = {'labelpad':14, 'fontsize':14}\n",
    "\n",
    "\n",
    "# # use these to format date axis in charts\n",
    "# weeks = mdates.WeekdayLocator(byweekday=1, interval=4)\n",
    "# # onedayweek = mdates.DayLocator(bymonthday=1, interval=1)\n",
    "# # everytwoweeks = mdates.WeekdayLocator(byweekday=1, interval=4)\n",
    "\n",
    "# months = mdates.MonthLocator(bymonth=[3,6,9,12])\n",
    "# bimonthly = mdates.MonthLocator(bymonth=[1,3,5,7,9,11])\n",
    "# allmonths = mdates.MonthLocator()\n",
    "# wks_fmt = mdates.DateFormatter('%d')\n",
    "# mths_fmt = mdates.DateFormatter('%b')\n",
    "\n",
    "# map marker size:\n",
    "markerSize = 100\n",
    "survey_data, location_data, code_defs, stat_ent, geo_data, output = ut.make_local_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some parameters:\n",
    "start_date = '2020-04-01'\n",
    "end_date = dt.datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "startyearmonth = '{}/{}'.format(start_date[5:7], start_date[:4])\n",
    "endyearmonth = '{}/{}'.format(end_date[5:7], end_date[:4]) \n",
    "\n",
    "# decide which data to use\n",
    "aggregated = False\n",
    "\n",
    "\n",
    "# collect the names:\n",
    "# group_names = list(these_groups.keys())\n",
    "\n",
    "# choose a lake:\n",
    "# lake = 'Lac Léman'\n",
    "coi = 'Biel/Bienne'\n",
    "bassin_label = 'Aare'\n",
    "bassin = ['Aare', 'Aare|Nidau-Büren-Kanal','Schüss', 'Neuenburgersee', 'Thunersee','Bielersee', 'Brienzersee','La Thièle']\n",
    "bassin_lmn = ['Rhône', 'Lac Léman']\n",
    "samples_all = 'All samples'\n",
    "# lavey_locs= ['lavey-les-bains-2','lavey-les-bains', 'lavey-la-source']\n",
    "\n",
    "\n",
    "# define a significant event:\n",
    "sig = .9\n",
    "one_minus_sig = (1-sig)\n",
    "\n",
    "# define explanatory variables:\n",
    "expv = ['population','streets','buildings','rivs']\n",
    "\n",
    "# name the folder:\n",
    "name_of_project = 'bielsummary'\n",
    "save_output = False\n",
    "\n",
    "# use this to store things:\n",
    "project_directory = ut.make_project_folder(output, name_of_project)\n",
    "\n",
    "# get the data\n",
    "# aggregated survey data\n",
    "# dfAgg = pd.read_csv(F\"{survey_data}/results_with_zeroes_aggregated_parent.csv\")\n",
    "\n",
    "files_generated = []\n",
    "figure_num = 1\n",
    "data_num = 1\n",
    "\n",
    "def add_output(**kwargs):\n",
    "    files_generated.append({'tag':kwargs['tag'], 'number':kwargs['figure_num'], 'file':kwargs['file'],'type':kwargs['a_type']})\n",
    "    if kwargs['a_type'] == 'data':\n",
    "        kwargs['data'].to_csv(F\"{kwargs['file']}.csv\", index=False)\n",
    "    else:\n",
    "        plt.savefig(F\"{kwargs['file']}.jpeg\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non aggregated survey data\n",
    "dfSurveys = pd.read_csv(F\"{survey_data}/results_with_zeroes.csv\")\n",
    "dfSurveys['date'] = pd.to_datetime(dfSurveys['date'])\n",
    "dfSurveys = dfSurveys[dfSurveys.date >= start_date]\n",
    "dfSurveys['groupname'] = 'nogroup'\n",
    "\n",
    "# beach data\n",
    "dfBeaches = pd.read_csv(F\"{location_data}/beaches_with_ranks.csv\")\n",
    "dfBeaches.set_index('slug', inplace=True)\n",
    "dfBeaches.rename(columns={\"NUMPOINTS\":\"intersects\"}, inplace=True)\n",
    "\n",
    "# code definitions\n",
    "dfCodes = pd.read_csv(F\"{code_defs}/mlw_codes.csv\", index_col='code')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "group_names_locations = {\n",
    "    \"waste water\": \"wastewater.json\" ,\n",
    "    \"less than 5mm\":\"codeListMicros.json\",\n",
    "    \"construction\":\"construction.json\",\n",
    "    \"food\":\"foodstuff.json\",\n",
    "    \"agg-con-trans\":\"cat.json\",\n",
    "    \"agriculture\":\"ag.json\",\n",
    "    \"tobacco\":\"tobac.json\",\n",
    "    \"recreation\":\"recreation.json\",    \n",
    "    \"packaging\":\"packaging.json\",\n",
    "    \"personal items\":\"pi.json\",    \n",
    "}\n",
    "frag_plas = {\"fragmented plastics\":[\"G79\", \"G78\", \"G75\"]}\n",
    "levels={'muni':coi, 'catchment':bassin_label}\n",
    "\n",
    "these_cols = ['loc_date', 'location', 'water_name','type', 'date']\n",
    "catchment_cols = ['region','water_name','type','city','loc_date', 'location', 'date']\n",
    "foams={'G82':['G82', 'G912'], 'G81':['G81', 'G911'], 'G74':['G74', 'G910', 'G909']}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dfSurveys.loc[(dfSurveys.date >= start_date)&(dfSurveys.date <= end_date)].copy()\n",
    "\n",
    "def use_apply_meth(x):\n",
    "    return dfBeaches[dfBeaches.index == x]['water'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['type'] = data.location.apply(use_apply_meth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'code', 'pcs_m', 'quantity', 'location', 'loc_date',\n",
       "       'water_name', 'groupname', 'type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         l\n",
       "1         l\n",
       "2         l\n",
       "3         l\n",
       "4         l\n",
       "         ..\n",
       "131483    r\n",
       "131484    r\n",
       "131485    r\n",
       "131486    r\n",
       "131487    r\n",
       "Name: type, Length: 74144, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Zurichsee', 'Aare', 'Aare|Nidau-Büren-Kanal', 'Lac Léman', 'Arve',\n",
       "       'Lago Maggiore', 'Thunersee', 'Untersee', 'Bielersee', 'Birs',\n",
       "       'Bodensee', 'Ticino', 'Chriesbach', 'Neuenburgersee', 'Emme',\n",
       "       'Walensee', 'Glatt', 'Goldach', 'Greifensee', 'Grändelbach',\n",
       "       'Brienzersee', 'Inn', 'Jona', 'Katzensee', 'Dorfbach', 'La Thièle',\n",
       "       'Langeten', 'Rhône', 'Limmat', 'Linthkanal', 'Escherkanal',\n",
       "       'Lorze', 'Lötschebach', 'Murg', 'Ognonnaz', 'Pfaffnern', 'Reuss',\n",
       "       'Rhein', 'Maggia', 'Schiffenensee', 'Schüss', 'Seez', 'Sempachsee',\n",
       "       'Sense', 'Sihlsee', 'Sihl', 'Sitter', 'Thur', 'Töss', 'Urnäsch',\n",
       "       'Quatre Cantons', 'Vorderrhein', 'Zugersee', 'Zulg'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfBeaches.water_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code maps done\n",
      "agg foams complet\n",
      "added exp vs\n"
     ]
    }
   ],
   "source": [
    "a = ac.PreprocessData(data, dfBeaches,these_cols=these_cols, foams=foams, start_date=start_date, end_date=end_date)\n",
    "\n",
    "clas_kwargs = dict(\n",
    "    code_group_data=group_names_locations,\n",
    "    new_code_group=frag_plas,\n",
    "    levels=levels,\n",
    "    catchment_features=bassin,\n",
    "    end_date=end_date,\n",
    "    start_date=start_date,\n",
    "    code_group_loc=output,\n",
    "    catchment_cols=catchment_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = ac.CatchmentArea(a.processed, dfBeaches, **clas_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "clas_kwargs.update({'catchment_features':bassin_lmn})\n",
    "c = ac.CatchmentArea(a.processed, dfBeaches, **clas_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'code', 'pcs_m', 'quantity', 'location', 'loc_date',\n",
      "       'water_name', 'groupname', 'type', 'population', 'string_date',\n",
      "       'region', 'city', 'fail'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "som_bassin_data = b.bassin_data.copy()\n",
    "som_bassin_data['fail'] = som_bassin_data.quantity > 0\n",
    "print(som_bassin_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map number of samples per location and sample total of all objects to each record\n",
    "\n",
    "def add_attribute(x, a_map={}):\n",
    "    try:\n",
    "        num_samps = a_map[x]\n",
    "    except:\n",
    "        num_samps = 0\n",
    "    return num_samps\n",
    "\n",
    "# conut the number of samples per location\n",
    "num_samp_location = b.bassin_pcsm_day.groupby('location').loc_date.count()\n",
    "\n",
    "# retrieve the total quantity per sample\n",
    "qty_sample = b.bassin_pcsm_day.groupby('loc_date').quantity.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map values to bassin data\n",
    "\n",
    "# the number of samples at a location\n",
    "som_bassin_data['nsamps'] = som_bassin_data.location.apply(add_attribute, a_map=num_samp_location)\n",
    "\n",
    "# the number of samples at a location divided by the total number of samples\n",
    "som_bassin_data['l_weight'] = som_bassin_data.nsamps/len(som_bassin_data.loc_date.unique())\n",
    "\n",
    "# the survey total for the corresponding loc_date variable\n",
    "som_bassin_data['samp_total'] = som_bassin_data.loc_date.map(lambda x: qty_sample[[x]][0])\n",
    "\n",
    "# the % of total for that object for the survey defined by loc_date\n",
    "som_bassin_data['% of total'] = som_bassin_data.quantity/som_bassin_data.samp_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([('la-petite-plage', '2021-03-12'),\n",
       "       ('la-petite-plage', '2021-02-15'),\n",
       "       ('la-petite-plage', '2021-01-16'),\n",
       "       ('la-petite-plage', '2020-12-14'),\n",
       "       ('la-petite-plage', '2020-11-15')], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "som_bassin_data.loc_date.unique()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12311\n"
     ]
    }
   ],
   "source": [
    "sbd = som_bassin_data.copy()\n",
    "\n",
    "def save_describe(x):\n",
    "    data=x.describe()\n",
    "    return data.values\n",
    "\n",
    "# save the key values from the original data:\n",
    "q_before = save_describe(som_bassin_data.quantity)\n",
    "pc_befor = save_describe(som_bassin_data.pcs_m)\n",
    "print(som_bassin_data.quantity.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank the amplitude of quantity of each code in each survey for all locations\n",
    "ranked = []\n",
    "\n",
    "# the loc_date values to aggregate:\n",
    "locs = som_bassin_data.loc_date.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    27500.000000\n",
       "mean         0.447673\n",
       "std          3.342633\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          0.000000\n",
       "max        161.000000\n",
       "Name: quantity, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "som_bassin_data.quantity.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    27500.000000\n",
       "mean         0.010402\n",
       "std          0.078886\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          0.000000\n",
       "max          3.230000\n",
       "Name: pcs_m, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "som_bassin_data.pcs_m.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True  True  True]\n",
      "12311\n",
      "[ True False  True  True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "# for each value in loc_date:\n",
    "\n",
    "for a_samp in locs:\n",
    "    # sort the values for each survey by \n",
    "    sbdl = sbd[sbd.loc_date == a_samp].sort_values(by='quantity', ascending=False)\n",
    "    sbdl = sbdl.set_index(np.arange(1, len(sbdl)+1), drop=True)\n",
    "    sbdl['rank'] = sbdl.index\n",
    "    ranked.append(sbdl)\n",
    "    \n",
    "# new data with amplitude rankings for each record\n",
    "som_bassin_data = pd.concat(ranked, ignore_index=True)\n",
    "\n",
    "# check the key values before and after:\n",
    "print(q_before == som_bassin_data.quantity.describe().values)\n",
    "print(som_bassin_data.quantity.sum())\n",
    "print(pc_befor == som_bassin_data.pcs_m.describe().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check the n_weight sum:0.9999999999999998\n",
      "\n",
      "Check the % of total column: 1.0\n",
      "\n",
      "The number of times that G27 is ranked one: 46\n",
      "\n",
      "The aggregated values should be the same: 46\n",
      "\n",
      "The locations where G27 was number one at least once: \n",
      "\n",
      "['aare-solothurn-lido-strand' 'aare_brugg_buchie' 'aare_koniz_hoppej'\n",
      " 'aare_post' 'camping-gwatt-strand' 'evole-plage' 'hafeli'\n",
      " 'hauterive-petite-plage' 'la-petite-plage' 'mullermatte'\n",
      " 'plage-de-cheyres' 'ruisseau-de-la-croix-plage' 'schusspark-strand'\n",
      " 'sundbach-strand' 'thun-strandbad' 'weissenau-neuhaus']\n",
      "\n",
      "G27 has been the number one at least once at 36.0% of the locations sampled\n",
      "\n",
      "G27 has been the number one object at a survey 37.0% of the time\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group by code and location. keep the columns that are specific to the location\n",
    "# agg results: pieces per meter and quantity. count the number of samples for this code and location\n",
    "sbd_ranked = som_bassin_data.groupby(['code','rank','location', 'population', 'groupname','l_weight'], as_index=False).agg({'pcs_m':'median', 'loc_date':'count', 'quantity':'sum'})\n",
    "\n",
    "# recalculate the % of total for the group levels\n",
    "# the sum of this col should equal one\n",
    "sbd_ranked['% of total'] = sbd_ranked.quantity/sbd_ranked.quantity.sum()\n",
    "\n",
    "# house keeping\n",
    "# the number of surveys where this row value would be tru\n",
    "sbd_ranked.rename(columns={'loc_date':'n_times'}, inplace=True)\n",
    "\n",
    "# calculate the weight of the sample in reference to all other samples\n",
    "# the sum of rank_weight should equal one\n",
    "sbd_ranked['n_weight'] = sbd_ranked.n_times/sbd_ranked.n_times.sum()\n",
    "\n",
    "print(F\"Check the n_weight sum:{sbd_ranked.n_weight.sum()}\\n\\nCheck the % of total column: {sbd_ranked['% of total'].sum()}\\n\")\n",
    "\n",
    "print(F\"The number of times that G27 is ranked one: {len(som_bassin_data[(som_bassin_data.code == 'G27')&(som_bassin_data['rank'] == 1)])}\\n\")\n",
    "\n",
    "# aggregate and check:\n",
    "rank_and_code = sbd_ranked.groupby(['rank','code','location'], as_index=False).n_times.sum()\n",
    "print(F\"The aggregated values should be the same: {rank_and_code[(rank_and_code.code == 'G27')&(rank_and_code['rank'] == 1)].n_times.sum()}\\n\")\n",
    "\n",
    "print(F\"The locations where G27 was number one at least once: \\n\\n{rank_and_code[(rank_and_code.code == 'G27')&(rank_and_code['rank'] == 1)].location.unique()}\")\n",
    "print(F\"\\nG27 has been the number one at least once at {np.round((len(rank_and_code[(rank_and_code.code == 'G27')&(rank_and_code['rank'] == 1)].location.unique())/len(som_bassin_data.location.unique())*100),0)}% of the locations sampled\")\n",
    "\n",
    "print(F\"\\nG27 has been the number one object at a survey {np.round((len(som_bassin_data[(som_bassin_data.code == 'G27')&(som_bassin_data['rank'] == 1)])/len(som_bassin_data.loc_date.unique()))*100, 0)}% of the time\\n\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Codes and the number of times that they have been the number one item at a survey\n",
      "code\n",
      "G27     46\n",
      "G200    17\n",
      "G78     11\n",
      "G30      9\n",
      "G81      8\n",
      "G79      7\n",
      "G67      7\n",
      "G117     3\n",
      "G941     2\n",
      "G98      2\n",
      "G74      2\n",
      "G33      1\n",
      "G213     1\n",
      "G208     1\n",
      "G203     1\n",
      "G201     1\n",
      "G87      1\n",
      "G178     1\n",
      "G146     1\n",
      "G131     1\n",
      "G923     1\n",
      "G112     1\n",
      "Name: n_times, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "other_number_one_ranks = rank_and_code[rank_and_code['rank'] == 1].groupby(['code','location'], as_index=False).n_times.sum()\n",
    "print(F\"Codes and the number of times that they have been the number one item at a survey\\n{other_number_one_ranks.groupby('code').n_times.sum().sort_values(ascending=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project directory is 'output/bielsummary'\n",
    "sbd_ranked.to_csv(F\"{project_directory}/aare_codes_ranked.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "these_codes = ['G27', 'G30','G25', 'G81', 'G95', 'G67','G82']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12311\n"
     ]
    }
   ],
   "source": [
    "sbd_ranked_gs = som_bassin_data.groupby(['loc_date','groupname','location', 'population'], as_index=False).agg({'pcs_m':'sum', 'quantity':'sum'})\n",
    "\n",
    "sbd_ranked_gs['% of total'] = sbd_ranked_gs.apply(lambda x: x.quantity/qty_sample[x.loc_date], axis=1)\n",
    "# save the key values from the original data:\n",
    "q_before = save_describe(sbd_ranked_gs.quantity)\n",
    "pc_befor = save_describe(sbd_ranked_gs.pcs_m)\n",
    "print(sbd_ranked_gs.quantity.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check that the survey values have not been altered:\n",
      "\n",
      "[ True  True False  True  True  True  True  True]\n",
      "[ True  True  True  True  True  True  True  True]\n",
      "\n",
      "Check % of total: 125.0\n",
      "\n",
      "The number of SURVEYS where ALL TOBACCO items combined are rank one: 33\n",
      "\n",
      "All TOBACCO items combined were number one in 26.0% of the surveys.\n",
      "\n",
      "The number of locations that tobaco is rank one: 11\n",
      "\n",
      "The objects labeled tobacco:\n",
      "['G152' 'G25' 'G26' 'G27']\n"
     ]
    }
   ],
   "source": [
    "ranked = []\n",
    "for a_samp in locs:\n",
    "    sbdl = sbd_ranked_gs[sbd_ranked_gs.loc_date == a_samp].sort_values(by='% of total', ascending=False)\n",
    "    sbdl = sbdl.set_index(np.arange(1, len(sbdl)+1), drop=True)\n",
    "    sbdl['rank'] = sbdl.index\n",
    "    ranked.append(sbdl)\n",
    "    \n",
    "sbd_ranked_gs = pd.concat(ranked, ignore_index=True)\n",
    "\n",
    "# check the key values before and after:\n",
    "print(\"Check that the survey values have not been altered:\\n\")\n",
    "print(q_before == sbd_ranked_gs.quantity.describe().values)\n",
    "print(pc_befor == sbd_ranked_gs.pcs_m.describe().values)\n",
    "\n",
    "# check the % of total\n",
    "print(F\"\\nCheck % of total: {sbd_ranked_gs.groupby('loc_date')['% of total'].sum().sum()}\\n\")\n",
    "print(F\"The number of SURVEYS where ALL TOBACCO items combined are rank one: {sbd_ranked_gs[(sbd_ranked_gs.groupname == 'tobacco')&(sbd_ranked_gs['rank'] == 1)].loc_date.nunique()}\\n\")\n",
    "\n",
    "print(F\"All TOBACCO items combined were number one in {np.round((sbd_ranked_gs[(sbd_ranked_gs.groupname == 'tobacco')&(sbd_ranked_gs['rank'] == 1)].loc_date.nunique()/sbd_ranked_gs.loc_date.nunique())*100,0)}% of the surveys.\")\n",
    "print(F\"\\nThe number of locations that tobaco is rank one: {sbd_ranked_gs[(sbd_ranked_gs.groupname == 'tobacco')&(sbd_ranked_gs['rank'] == 1)].location.nunique()}\\n\")\n",
    "print(F\"The objects labeled tobacco:\\n{sbd_ranked[sbd_ranked.groupname == 'tobacco'].code.unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The number of samples: 125\n",
      "\n",
      "Other groups and the number of times they have been the most prevalent item:\n",
      "groupname\n",
      "food                   45\n",
      "tobacco                33\n",
      "fragmented plastics    18\n",
      "construction           14\n",
      "agg-con-trans           5\n",
      "less than 5mm           3\n",
      "waste water             2\n",
      "the rest                2\n",
      "recreation              2\n",
      "packaging               1\n",
      "Name: loc_date, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = sbd_ranked_gs.groupby(['rank','groupname','location'], as_index=False).loc_date.nunique()\n",
    "fx = f[f['rank'] == 1].groupby('groupname').loc_date.sum()\n",
    "print(F\"\\nThe number of samples: {fx.sum()}\\n\")\n",
    "print(F\"Other groups and the number of times they have been the most prevalent item:\\n{fx.sort_values(ascending=False)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_before[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sbd_ranked_gs[(sbd_ranked_gs['rank'] == 1)&(sbd_ranked_gs.groupname == 'tobacco')].groupby(['loc_date', 'groupname']).pcs_m.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbd_ranked_gl = sbd_ranked_g.groupby(['groupname','location', 'population','rank'], as_index=False).agg({'pcs_m':'median', '% of total':'median','loc_date':'count'})\n",
    "\n",
    "sbd_ranked_gl.rename(columns={'loc_date':'n_times'}, inplace=True)\n",
    "\n",
    "randg = sbd_ranked_gl[['rank','groupname']].value_counts()\n",
    "\n",
    "len(sbd_ranked_gl[(sbd_ranked_gl.groupname == 'tobacco')&(sbd_ranked_gl['rank'] == 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbd_ranked_gl.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok['a_rate'] = ok[\"n_times\"]/len(sbd_ranked_g.loc_date.unique())\n",
    "ok[ok['rank'] == 1].n_times.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som_params = som_bassin_data[som_bassin_data.quantity > 0].groupby(['location','code','l_weight','% of total'], as_index=False).agg({'pcs_m':'median', '% of total':'median'})\n",
    "som_params['w_pcsm'] = som_params.l_weight * som_params.pcs_m\n",
    "\n",
    "som_params[som_params.code == 'G27']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aone = som_params[som_params.code == 'G27'].l_weight.mean()\n",
    "loc = som_params[som_params.code == 'G27'].pcs_m.min()\n",
    "loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "an_alpha = som_params[som_params.code == 'G27'].pcs_m.mean()\n",
    "an_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1/som_params[som_params.code == 'G27'].pcs_m.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = gamma.rvs(an_alpha, loc=loc, scale=scale,  size=len(data))\n",
    "data = som_bassin_data[(som_bassin_data.code == 'G27')&(som_bassin_data.quantity > 0)].pcs_m\n",
    "k = gamma.rvs(an_alpha, loc=loc, size=len(data))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.histplot(data, ax=ax, color='blue', stat='probability')\n",
    "sns.histplot(r, ax=ax, color='red', stat='probability')\n",
    "# sns.histplot(k, ax=ax, color='yellow', stat='probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.processed.groupname.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tries = som_bassin_data.groupby('code').loc_date.nunique()\n",
    "fails = som_bassin_data.groupby('code').fail.sum()\n",
    "tries_fails = pd.concat([tries, fails], axis=1)\n",
    "tries_fails['rate'] = tries_fails.fail/tries_fails.loc_date\n",
    "asamp = tries_fails.loc[code].rate\n",
    "grtr_than_zero = som_bassin_data[(som_bassin_data.quantity > 0)].groupby(['code', 'location'], as_index=False).loc_date.count()\n",
    "# som_data = som_bassin_data.copy()\n",
    "# codes_in_use = grtr_than_zero.code.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grtr_than_zero.loc_date.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grtr_than_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_obs = som_data.groupby('type', as_index=False).date.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = 'G81'\n",
    "data = som_data[(som_data.code == code)&(som_data.quantity > 0)]['pcs_m'].copy()\n",
    "grtr_than_zero['weight'] = grtr_than_zero.date/grtr_than_zero.date.sum()\n",
    "print(list(np.cumsum(grtr_than_zero[grtr_than_zero.code == code].weight.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grtr_than_zero[grtr_than_zero.code == code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = tries_fails.loc[code].rate\n",
    "found = binom.rvs(1, rate,size=1)\n",
    "\n",
    "# this should give a\n",
    "\n",
    "d_lin = np.linspace(data.min(), data.max(), num=1000, dtype=np.float64)\n",
    "prior_params = gamma.fit(data[data >0])\n",
    "print(rate)\n",
    "print(found)\n",
    "print(prior_params)\n",
    "\n",
    "out = gamma.rvs(prior_params[0], loc=prior_params[1], scale=prior_params[2], size=len(d_lin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.histplot(data, ax=ax, color='blue', stat='probability')\n",
    "sns.histplot(out[out >0], ax=ax, color='red', stat='probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.min(), data.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.median())\n",
    "print(np.median(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(codes_in_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_n = fails.loc[code]\n",
    "this_p = rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {}\n",
    "\n",
    "for i in np.arange(125):\n",
    "\n",
    "    a_sample = {}\n",
    "    codes_found = []\n",
    "    for code in codes_in_use:\n",
    "        rate = tries_fails.loc[code].rate\n",
    "        found = binom.rvs(1, rate,size=1)[0]\n",
    "        if found:\n",
    "            codes_found.append(code)\n",
    "    samples.update({i:codes_found})\n",
    "            \n",
    "#             data = som_data[(som_data.code == code)]['pcs_m']\n",
    "#             if len(data) >= 10:\n",
    "#                 prior_params = gamma.fit(data)\n",
    "#                 alpha, loc, scale = prior_params\n",
    "#                 prior_dist = gamma(alpha, loc, scale)\n",
    "#                 theta_prior = prior_dist.rvs(1)\n",
    "#                 sum_samples.append({'code':code, 'pcs_m':theta_prior[0], 'date':i})\n",
    "\n",
    "#             else:\n",
    "#                 weights = grtr_than_zero[(grtr_than_zero.code == code)].copy()\n",
    "#                 weights['weight'] = weights.date/weights.date.sum()\n",
    "#                 theta_prior = choices(weights.pcs_m.values, cum_weights = np.cumsum(weights.weight.values))[0]\n",
    "#                 sum_samples.append({'code':code, 'pcs_m':theta_prior, 'date':i})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([len(v) for k,v in samples.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hopefully that just worked for you\n",
    "\n",
    "if not contact analyst@hammerdirt.ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
