{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys, file and nav packages:\n",
    "import os\n",
    "import datetime as dt\n",
    "import csv\n",
    "\n",
    "# math packages:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import datetime as dt \n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from scipy.stats import kendalltau, pearsonr, spearmanr\n",
    "import math\n",
    "\n",
    "# charting:\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib import ticker\n",
    "import matplotlib.ticker as mtick\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from IPython.display import display, Markdown, Latex, HTML\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap, Colormap\n",
    "\n",
    "# mapping\n",
    "import geopandas as gpd\n",
    "\n",
    "# home brew utitilties\n",
    "import utilities.utility_functions as ut\n",
    "\n",
    "# documenting\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "\n",
    "# returns the p_value for each test\n",
    "def kendall_pval(x,y):\n",
    "    return kendalltau(x,y)[1]\n",
    "\n",
    "def pearsonr_pval(x,y):\n",
    "    return pearsonr(x,y)[1]\n",
    "\n",
    "def spearmanr_pval(x,y):\n",
    "    return spearmanr(x,y)[1]\n",
    "\n",
    "\n",
    "def make_ecdf(somdata, numsamps):\n",
    "    vals = somdata.pcs_m.sort_values()\n",
    "    valsy = [i/numsamps for i in np.arange(numsamps)]\n",
    "    return vals, valsy\n",
    "\n",
    "\n",
    "def assign_a_level(x, a_list, labels):\n",
    "    if x in a_list:\n",
    "        this_level = labels[0]\n",
    "    else:\n",
    "        this_level = labels[1]\n",
    "    return this_level\n",
    "\n",
    "def count_k(a_string, limit):\n",
    "    split = a_string.split(\" \")\n",
    "    total = 0\n",
    "    new_words = []\n",
    "    for i,word in enumerate(split):\n",
    "        if (total + len(word))+1 >= limit:\n",
    "            thisnewword = F\"{split[i-1]}...\"\n",
    "            if (len(thisnewword) + total) <= limit:\n",
    "                del new_words[-1]\n",
    "                new_words.append(thisnewword)\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            total += len(word)+1\n",
    "            new_words.append(word)\n",
    "\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "# convenience functions for tables\n",
    "\n",
    "def make_table_grids(anax):\n",
    "    anax.grid(False)\n",
    "    anax.spines[\"top\"].set_visible(False)\n",
    "    anax.spines[\"right\"].set_visible(False)\n",
    "    anax.spines[\"bottom\"].set_visible(False)\n",
    "    anax.spines[\"left\"].set_visible(False)\n",
    "    return(anax)\n",
    "\n",
    "def table_fonts(a_table, size=12):\n",
    "    a_table.auto_set_font_size(False)\n",
    "    a_table.set_fontsize(size)\n",
    "\n",
    "# table kwargs\n",
    "table_k = dict(loc=\"top left\", bbox=(0,0,1,1), colWidths=[.5, .5], cellLoc='center')\n",
    "tablecenter_k = dict(loc=\"top left\", bbox=(0,0,1,1), cellLoc='center')\n",
    "tabtickp_k = dict(axis='both', which='both', bottom=False, top=False, left=False, right=False, labelleft=False, labelbottom=False)\n",
    "\n",
    "# chart kwargs\n",
    "title_k = {'loc':'left', 'pad':14, 'linespacing':1.5, 'fontsize':12}\n",
    "title_k14 = {'loc':'left', 'pad':16, 'linespacing':1.5, 'fontsize':14}\n",
    "title_k20 = {'loc':'left', 'pad':10, 'linespacing':1.5, 'fontsize':12, 'color':'dodgerblue'}\n",
    "title_k17 = {'loc':'left', 'pad':10, 'linespacing':1.5, 'fontsize':12, 'color':'salmon'}\n",
    "titler_k20 = {'loc':'right', 'pad':10, 'linespacing':1.5, 'fontsize':12, 'color':'dodgerblue'}\n",
    "titler_k17 = {'loc':'right', 'pad':10, 'linespacing':1.5, 'fontsize':12, 'color':'salmon'}\n",
    "xlab_k = {'labelpad':10, 'fontsize':12}\n",
    "ylab_k = {'labelpad':14, 'fontsize':14}\n",
    "titler_k = {'loc':'right', 'pad':10, 'linespacing':1.5, 'fontsize':12}\n",
    "label45r = {'rotation':45, 'ha':'right'}\n",
    "label45c = {'rotation':45, 'ha':'center'}\n",
    "\n",
    "# use these to format date axis in charts\n",
    "weeks = mdates.WeekdayLocator(byweekday=1, interval=4)\n",
    "onedayweek = mdates.DayLocator(bymonthday=1, interval=1)\n",
    "everytwoweeks = mdates.WeekdayLocator(byweekday=1, interval=4)\n",
    "\n",
    "months = mdates.MonthLocator(bymonth=[3,6,9,12])\n",
    "bimonthly = mdates.MonthLocator(bymonth=[1,3,5,7,9,11])\n",
    "allmonths = mdates.MonthLocator()\n",
    "wks_fmt = mdates.DateFormatter('%d')\n",
    "mths_fmt = mdates.DateFormatter('%b')\n",
    "\n",
    "# map marker size:\n",
    "markerSize = 100\n",
    "survey_data, location_data, code_defs, stat_ent, geo_data, output = ut.make_local_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def make_group_map(a_dict_of_lists):\n",
    "#     wiw = {}\n",
    "#     for group in a_dict_of_lists:\n",
    "#         keys = a_dict_of_lists[group]\n",
    "#         a_dict = {x:group for x in keys}\n",
    "#         wiw.update(**a_dict)\n",
    "#     return wiw\n",
    "\n",
    "# these_groups ={k:ut.json_file_get(F\"{output}/code_groups/{v}\") for k,v in som_names.items()}\n",
    "# these_groups.update({\"fragmented plastics\":[\"G79\", \"G78\", \"G75\"]})\n",
    "# group_names = list(these_groups.keys())\n",
    "\n",
    "# # collect the codes\n",
    "# accounted = [v for k,v in these_groups.items()]\n",
    "# accounted = [item for a_list in accounted for item in a_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some parameters:\n",
    "start_date = '2020-04-01'\n",
    "end_date = dt.datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "startyearmonth = '{}/{}'.format(start_date[5:7], start_date[:4])\n",
    "endyearmonth = '{}/{}'.format(end_date[5:7], end_date[:4]) \n",
    "\n",
    "# decide which data to use\n",
    "aggregated = False\n",
    "\n",
    "\n",
    "# collect the names:\n",
    "# group_names = list(these_groups.keys())\n",
    "\n",
    "# choose a lake:\n",
    "# lake = 'Lac Léman'\n",
    "coi = 'Zürich'\n",
    "bassin_label = 'Limmat'\n",
    "bassin = ['Limmat', 'Linthkanal', 'Escherkanal', 'Seez', 'Zurichsee', 'Sihl', 'Jona']\n",
    "# lavey_locs= ['lavey-les-bains-2','lavey-les-bains', 'lavey-la-source']\n",
    "\n",
    "\n",
    "# define a significant event:\n",
    "sig = .9\n",
    "one_minus_sig = (1-sig)\n",
    "\n",
    "# define explanatory variables:\n",
    "expv = ['population','streets','buildings','rivs']\n",
    "\n",
    "# name the folder:\n",
    "name_of_project = 'laveysummary'\n",
    "\n",
    "# use this to store things:\n",
    "project_directory = ut.make_project_folder(output, name_of_project)\n",
    "\n",
    "# get the data\n",
    "# aggregated survey data\n",
    "# dfAgg = pd.read_csv(F\"{survey_data}/results_with_zeroes_aggregated_parent.csv\")\n",
    "\n",
    "files_generated = []\n",
    "\n",
    "# method to save\n",
    "def add_output(a_name, a_tag, atype=\"table\", fignum=0, a_list=files_generated, data=[]):\n",
    "    tableonefile = F\"{project_directory}/{a_name}\"\n",
    "    files_generated.append({'tag':a_tag, 'number':fignum, 'file':tableonefile,'type':atype})\n",
    "    if atype == 'data':\n",
    "        data.to_csv(a_name, index=False)\n",
    "    else:\n",
    "        plt.savefig(tableonefile, dpi=300)\n",
    "\n",
    "# save files\n",
    "# survey_csv = F\"{project_directory}/survey_data.csv\"\n",
    "\n",
    "# files_generated.append(survey_csv)\n",
    "# useThis.to_csv(survey_csv, index=False)\n",
    "# data_num = 1\n",
    "# figname = F\"{project_directory}/survey_data.csv\"\n",
    "# atype = \"data\"\n",
    "# data = useThis\n",
    "# atag = 'All survey data'\n",
    "# add_output(figname, atag, fignum=data_num, atype=atype, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non aggregated survey data\n",
    "dfSurveys = pd.read_csv(F\"{survey_data}/results_with_zeroes.csv\")\n",
    "dfSurveys['date'] = pd.to_datetime(dfSurveys['date'])\n",
    "dfSurveys = dfSurveys[dfSurveys.date >= start_date]\n",
    "dfSurveys['groupname'] = 'nogroup'\n",
    "\n",
    "# beach data\n",
    "dfBeaches = pd.read_csv(F\"{location_data}/beaches_with_ranks.csv\")\n",
    "dfBeaches.set_index('slug', inplace=True)\n",
    "dfBeaches.rename(columns={\"NUMPOINTS\":\"intersects\"}, inplace=True)\n",
    "\n",
    "# code definitions\n",
    "dfCodes = pd.read_csv(F\"{code_defs}/mlw_codes.csv\", index_col='code')\n",
    "\n",
    "group_names_locations = {\n",
    "    \"waste water\": \"wastewater.json\" ,\n",
    "    \"less than 5mm\":\"codeListMicros.json\",\n",
    "    \"construction\":\"construction.json\",\n",
    "    \"food\":\"foodstuff.json\",\n",
    "    \"agg-con-trans\":\"cat.json\",\n",
    "    \"agriculture\":\"ag.json\",\n",
    "    \"tobacco\":\"tobac.json\",\n",
    "    \"recreation\":\"recreation.json\",    \n",
    "    \"packaging\":\"packaging.json\",\n",
    "    \"personal items\":\"pi.json\",    \n",
    "}\n",
    "frag_plas = {\"fragmented plastics\":[\"G79\", \"G78\", \"G75\"]}\n",
    "group_cols = ['loc_date', 'date','location','water_name', 'groupname']\n",
    "levels={'muni':coi, 'catchment':bassin_label}\n",
    "\n",
    "these_cols = ['loc_date', 'location', 'water_name', 'date']\n",
    "foams={'G82':['G82', 'G912'], 'G81':['G81', 'G911'], 'G74':['G74', 'G910', 'G909']}\n",
    "\n",
    "class PreprocessData:\n",
    "    \"\"\"preprocesses data\"\"\"\n",
    "    def __init__(self, data, beaches, these_cols=these_cols, foams=foams, start_date=start_date, end_date=end_date):\n",
    "        self.data = data.loc[(data.date >= start_date)&(data.date <= end_date)].copy()\n",
    "        self.these_cols=these_cols\n",
    "        self.foams=foams        \n",
    "        self.beaches = beaches\n",
    "        self.code_maps = self.make_code_maps(self.data, self.these_cols, self.foams)\n",
    "        self.processed = self.add_exp_group_pop_locdate()\n",
    "    def make_code_maps(self, data, these_cols, these_codes):\n",
    "        wiw = {}\n",
    "        for code in these_codes:\n",
    "            a_map = data[data.code.isin(these_codes[code])].groupby(these_cols, as_index=False).agg({'pcs_m':'sum', 'quantity':'sum'})\n",
    "            a_map['code']=code\n",
    "            wiw.update({code:a_map})\n",
    "        return wiw\n",
    "    def agg_foams(self):\n",
    "#         code_maps = self.make_code_maps(self.data, self.these_cols, self.foams)\n",
    "        accounted = [v for k,v in self.foams.items()]\n",
    "        accounted = [item for a_list in accounted for item in a_list]\n",
    "        remove_foam = self.data[~self.data.code.isin(accounted)].copy()\n",
    "        foam = [v for k,v in self.code_maps.items()]        \n",
    "        newdf = pd.concat([remove_foam, *foam])        \n",
    "        return newdf\n",
    "    def add_exp_group_pop_locdate(self):\n",
    "        anewdf = self.agg_foams()\n",
    "        anewdf['groupname'] = 'groupname'\n",
    "        anewdf['population']=anewdf.location.map(lambda x: self.beaches.loc[x]['population'])\n",
    "        anewdf['loc_date'] = list(zip(anewdf.location, anewdf.date))\n",
    "        return anewdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46773"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = PreprocessData(dfSurveys.copy(), dfBeaches)\n",
    "b = a.processed\n",
    "b.quantity.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46773"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfSurveys.quantity.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clas_kwargs = dict(\n",
    "    code_group_data=group_names_locations,\n",
    "    new_code_group=frag_plas,\n",
    "    levels=levels,\n",
    "    catchment_features=bassin,\n",
    "    end_date=end_date,\n",
    "    start_date=start_date)\n",
    "\n",
    "class CatchmentArea:\n",
    "    \"\"\"aggregates survey results\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        these_beaches,\n",
    "        **kwargs):\n",
    "#         print(new_code_group)\n",
    "        self.data = data\n",
    "        self.beaches = these_beaches\n",
    "        self.start_date = kwargs['start_date']\n",
    "        self.end_date = kwargs['end_date']\n",
    "        self.levels = kwargs['levels']\n",
    "        self.catchment = self.levels['catchment']\n",
    "        self.muni = self.levels['muni']\n",
    "        self.locations_in_use = self.data.location.unique()\n",
    "        self.muni_beaches = self.get_locations_by_region(self.locations_in_use, self.beaches[self.beaches.city == self.muni].index)\n",
    "        self.catchment_features = kwargs['catchment_features']\n",
    "        self.bassin_beaches = self.get_locations_by_region(self.locations_in_use, self.beaches[self.beaches.water_name.isin(self.catchment_features)].index)        \n",
    "        self.codes_in_use = data.code.unique()\n",
    "        self.group_names_locations = kwargs['code_group_data']\n",
    "        self.new_code_group = kwargs['new_code_group']\n",
    "        self.code_groups = self.make_code_groups()\n",
    "        self.code_group_map = self.make_group_map(self.code_groups)\n",
    "        self.bassin_data = self.assign_regional_labels_to_data(self.assign_code_groups_to_results(data[data.location.isin(self.bassin_beaches)].copy(), self.code_group_map), self.levels, these_beaches)\n",
    "        self.muni_data = self.assign_regional_labels_to_data(self.assign_code_groups_to_results(data[data.location.isin(self.muni_beaches)].copy(), self.code_group_map), self.levels, these_beaches)\n",
    "        self.bassin_code_totals = self.code_totals_regional(self.bassin_data)\n",
    "        self.muni_code_totals = self.code_totals_regional(self.muni_data)\n",
    "        \n",
    "           \n",
    "    def make_group_map(self,a_dict_of_lists):\n",
    "        wiw = {}\n",
    "        for group in a_dict_of_lists:\n",
    "            keys = a_dict_of_lists[group]\n",
    "            a_dict = {x:group for x in keys}\n",
    "            wiw.update(**a_dict)\n",
    "        return wiw\n",
    "    \n",
    "    def make_code_groups(self):\n",
    "        these_groups ={k:ut.json_file_get(F\"{output}/code_groups/{v}\") for k,v in self.group_names_locations.items()}\n",
    "        these_groups.update(self.new_code_group)\n",
    "        accounted = [v for k,v in these_groups.items()]\n",
    "        accounted = [item for a_list in accounted for item in a_list]\n",
    "        the_rest = [x for x in self.codes_in_use if x not in accounted]\n",
    "        these_groups.update({'the rest':the_rest})\n",
    "        return these_groups\n",
    "    \n",
    "    def assign_code_groups_to_results(self, data, code_group_map):\n",
    "        data = data.copy()\n",
    "        data['groupname'] = data.code.map(lambda x: code_group_map[x])\n",
    "        return data\n",
    "    \n",
    "    def tag_regional_label(self,x, levels):\n",
    "        if x in self.muni_beaches:\n",
    "            a_label = self.muni\n",
    "        else:\n",
    "            a_label = self.catchment\n",
    "        return a_label\n",
    "    \n",
    "    def assign_regional_labels_to_data(self, data, levels, these_beaches):\n",
    "        data = data.copy()\n",
    "        data['region'] = data.location.map(lambda x: self.tag_regional_label(x, self.levels))\n",
    "        data['city'] = data.location.map(lambda x: these_beaches.loc[x]['city'])\n",
    "        return data\n",
    "    \n",
    "    def code_totals_regional(self, data):\n",
    "        data = data.groupby('code', as_index=False).quantity.sum()\n",
    "        a_total = data.quantity.sum()\n",
    "        data['% of total'] = data.quantity/a_total\n",
    "        return data \n",
    "    \n",
    "    def code_pcsm_regional(self,data):\n",
    "        return data.groupby('code').pcs_m.median()\n",
    "    \n",
    "    def survey_totals_regional(self, data):\n",
    "        return data.groupby(['region','water_name','loc_date', 'location', 'date'], as_index=False).agg({'pcs_m':'sum', 'quantity':'sum'})\n",
    "    \n",
    "    def get_locations_by_region(self, locations_in_use, locations_of_interest):        \n",
    "        return [x for x in locations_of_interest if x in locations_in_use]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = CatchmentArea(b, dfBeaches, **clas_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>code</th>\n",
       "      <th>pcs_m</th>\n",
       "      <th>quantity</th>\n",
       "      <th>location</th>\n",
       "      <th>loc_date</th>\n",
       "      <th>water_name</th>\n",
       "      <th>groupname</th>\n",
       "      <th>population</th>\n",
       "      <th>region</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21952</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>G3</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "      <td>pfafikon-bad</td>\n",
       "      <td>(pfafikon-bad, 2021-02-18 00:00:00)</td>\n",
       "      <td>Zurichsee</td>\n",
       "      <td>packaging</td>\n",
       "      <td>16391</td>\n",
       "      <td>Limmat</td>\n",
       "      <td>Freienbach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21953</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>G941</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "      <td>pfafikon-bad</td>\n",
       "      <td>(pfafikon-bad, 2021-02-18 00:00:00)</td>\n",
       "      <td>Zurichsee</td>\n",
       "      <td>packaging</td>\n",
       "      <td>16391</td>\n",
       "      <td>Limmat</td>\n",
       "      <td>Freienbach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21954</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>G33</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>pfafikon-bad</td>\n",
       "      <td>(pfafikon-bad, 2021-02-18 00:00:00)</td>\n",
       "      <td>Zurichsee</td>\n",
       "      <td>food</td>\n",
       "      <td>16391</td>\n",
       "      <td>Limmat</td>\n",
       "      <td>Freienbach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21955</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>G96</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>pfafikon-bad</td>\n",
       "      <td>(pfafikon-bad, 2021-02-18 00:00:00)</td>\n",
       "      <td>Zurichsee</td>\n",
       "      <td>waste water</td>\n",
       "      <td>16391</td>\n",
       "      <td>Limmat</td>\n",
       "      <td>Freienbach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21956</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>G67</td>\n",
       "      <td>0.15</td>\n",
       "      <td>12</td>\n",
       "      <td>pfafikon-bad</td>\n",
       "      <td>(pfafikon-bad, 2021-02-18 00:00:00)</td>\n",
       "      <td>Zurichsee</td>\n",
       "      <td>agg-con-trans</td>\n",
       "      <td>16391</td>\n",
       "      <td>Limmat</td>\n",
       "      <td>Freienbach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>2020-10-13</td>\n",
       "      <td>G74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>zurichsee_wollishofen_langendorfm</td>\n",
       "      <td>(zurichsee_wollishofen_langendorfm, 2020-10-13...</td>\n",
       "      <td>Zurichsee</td>\n",
       "      <td>construction</td>\n",
       "      <td>415367</td>\n",
       "      <td>Zürich</td>\n",
       "      <td>Zürich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>2020-11-12</td>\n",
       "      <td>G74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>zurichsee_wollishofen_langendorfm</td>\n",
       "      <td>(zurichsee_wollishofen_langendorfm, 2020-11-12...</td>\n",
       "      <td>Zurichsee</td>\n",
       "      <td>construction</td>\n",
       "      <td>415367</td>\n",
       "      <td>Zürich</td>\n",
       "      <td>Zürich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>2020-12-10</td>\n",
       "      <td>G74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>zurichsee_wollishofen_langendorfm</td>\n",
       "      <td>(zurichsee_wollishofen_langendorfm, 2020-12-10...</td>\n",
       "      <td>Zurichsee</td>\n",
       "      <td>construction</td>\n",
       "      <td>415367</td>\n",
       "      <td>Zürich</td>\n",
       "      <td>Zürich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>2021-01-10</td>\n",
       "      <td>G74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>zurichsee_wollishofen_langendorfm</td>\n",
       "      <td>(zurichsee_wollishofen_langendorfm, 2021-01-10...</td>\n",
       "      <td>Zurichsee</td>\n",
       "      <td>construction</td>\n",
       "      <td>415367</td>\n",
       "      <td>Zürich</td>\n",
       "      <td>Zürich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>2021-02-12</td>\n",
       "      <td>G74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>zurichsee_wollishofen_langendorfm</td>\n",
       "      <td>(zurichsee_wollishofen_langendorfm, 2021-02-12...</td>\n",
       "      <td>Zurichsee</td>\n",
       "      <td>construction</td>\n",
       "      <td>415367</td>\n",
       "      <td>Zürich</td>\n",
       "      <td>Zürich</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13640 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  code  pcs_m  quantity                           location  \\\n",
       "21952 2021-02-18    G3   0.05         4                       pfafikon-bad   \n",
       "21953 2021-02-18  G941   0.05         4                       pfafikon-bad   \n",
       "21954 2021-02-18   G33   0.01         1                       pfafikon-bad   \n",
       "21955 2021-02-18   G96   0.01         1                       pfafikon-bad   \n",
       "21956 2021-02-18   G67   0.15        12                       pfafikon-bad   \n",
       "...          ...   ...    ...       ...                                ...   \n",
       "315   2020-10-13   G74   0.00         0  zurichsee_wollishofen_langendorfm   \n",
       "316   2020-11-12   G74   0.00         0  zurichsee_wollishofen_langendorfm   \n",
       "317   2020-12-10   G74   0.00         0  zurichsee_wollishofen_langendorfm   \n",
       "318   2021-01-10   G74   0.00         0  zurichsee_wollishofen_langendorfm   \n",
       "319   2021-02-12   G74   0.00         0  zurichsee_wollishofen_langendorfm   \n",
       "\n",
       "                                                loc_date water_name  \\\n",
       "21952                (pfafikon-bad, 2021-02-18 00:00:00)  Zurichsee   \n",
       "21953                (pfafikon-bad, 2021-02-18 00:00:00)  Zurichsee   \n",
       "21954                (pfafikon-bad, 2021-02-18 00:00:00)  Zurichsee   \n",
       "21955                (pfafikon-bad, 2021-02-18 00:00:00)  Zurichsee   \n",
       "21956                (pfafikon-bad, 2021-02-18 00:00:00)  Zurichsee   \n",
       "...                                                  ...        ...   \n",
       "315    (zurichsee_wollishofen_langendorfm, 2020-10-13...  Zurichsee   \n",
       "316    (zurichsee_wollishofen_langendorfm, 2020-11-12...  Zurichsee   \n",
       "317    (zurichsee_wollishofen_langendorfm, 2020-12-10...  Zurichsee   \n",
       "318    (zurichsee_wollishofen_langendorfm, 2021-01-10...  Zurichsee   \n",
       "319    (zurichsee_wollishofen_langendorfm, 2021-02-12...  Zurichsee   \n",
       "\n",
       "           groupname  population  region        city  \n",
       "21952      packaging       16391  Limmat  Freienbach  \n",
       "21953      packaging       16391  Limmat  Freienbach  \n",
       "21954           food       16391  Limmat  Freienbach  \n",
       "21955    waste water       16391  Limmat  Freienbach  \n",
       "21956  agg-con-trans       16391  Limmat  Freienbach  \n",
       "...              ...         ...     ...         ...  \n",
       "315     construction      415367  Zürich      Zürich  \n",
       "316     construction      415367  Zürich      Zürich  \n",
       "317     construction      415367  Zürich      Zürich  \n",
       "318     construction      415367  Zürich      Zürich  \n",
       "319     construction      415367  Zürich      Zürich  \n",
       "\n",
       "[13640 rows x 11 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Table:\n",
    "    def __init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'useThis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-41a19a7f7504>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# scatter chart of all project surveys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# use all the surveys un aggregated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mallsurveys\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0museThis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loc_date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'location'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'water_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpcs_m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# survey_totals = useThis.groupby(['loc_date','location','water_name', 'date','population'], as_index=False).agg({\"pcs_m\":\"sum\", \"quantity\":\"sum\"})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'useThis' is not defined"
     ]
    }
   ],
   "source": [
    "# scatter chart of all project surveys\n",
    "# use all the surveys un aggregated\n",
    "allsurveys =  useThis.groupby(['loc_date', 'location', 'date', 'water_name'], as_index=False).pcs_m.sum()\n",
    "\n",
    "# survey_totals = useThis.groupby(['loc_date','location','water_name', 'date','population'], as_index=False).agg({\"pcs_m\":\"sum\", \"quantity\":\"sum\"})\n",
    "allsurveys_pcs_median = allsurveys.groupby('location').pcs_m.median()\n",
    "\n",
    "# identify lakes v/s rivers\n",
    "allsurveys['type'] = allsurveys.location.map(lambda x: dfBeaches[dfBeaches.index == x]['water'].values[0])\n",
    "\n",
    "# group the data for the regional levels\n",
    "lavey = allsurveys[(allsurveys.location.isin(lavey_locs))].groupby(['loc_date', 'location', 'date', 'water_name'], as_index=False).pcs_m.sum()\n",
    "bassin_versant = allsurveys[(allsurveys.location.isin(bassin_locs))].groupby(['loc_date', 'location', 'date', 'water_name'], as_index=False).pcs_m.sum()\n",
    "\n",
    "# count the number of rivers and lakes from all the samples\n",
    "v_counts = allsurveys['type'].value_counts()\n",
    "rivercount = int(v_counts['r'])\n",
    "lakecount = int(v_counts['l'])\n",
    "\n",
    "# make a table of key statisitics for the different levels:\n",
    "a_sum = pd.DataFrame(allsurveys.pcs_m.describe()[1:].round(2)).T\n",
    "a_sum_table = [[x] for x in a_sum.values[0]]\n",
    "rowLabels = [x for x in list(a_sum.columns)]\n",
    "\n",
    "# the bassin versant\n",
    "bassin_sum = pd.DataFrame(bassin_versant.pcs_m.describe()[1:].round(2)).T\n",
    "bassin_table = [[x] for x in bassin_sum.values[0]]\n",
    "bassinLabels = [x for x in list(bassin_sum.columns)]\n",
    "\n",
    "# the municipality\n",
    "lavey_sum = pd.DataFrame(lavey.pcs_m.describe()[1:].round(2)).T\n",
    "lavey_table = [[x] for x in lavey_sum.values[0]]\n",
    "laveyLabels = [x for x in list(lavey_sum.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust table kwargs\n",
    "tablecenter_k = dict(loc=\"center\", bbox=(0,0,1,1), cellLoc='center')\n",
    "\n",
    "fig = plt.figure(constrained_layout = False, figsize=(12,6))\n",
    "figure_num += 1\n",
    "\n",
    "# declare a grid\n",
    "gs = GridSpec(1, 7, figure=fig)\n",
    "\n",
    "# put an ax on it\n",
    "ax1 = fig.add_subplot(gs[5:])\n",
    "\n",
    "# the context matters for the row and column labels\n",
    "if is_french:\n",
    "    rowLabels = summary_row_fr\n",
    "    col_label = [french_pcm]\n",
    "else:\n",
    "    col_label = ['pieces per meter']\n",
    "\n",
    "# define the table\n",
    "a_table = mpl.table.table(\n",
    "    cellText=bassin_table,\n",
    "    rowLabels=rowLabels,\n",
    "    rowColours=['antiquewhite' for i in rowLabels],\n",
    "    colLabels=col_label,\n",
    "    colColours=['antiquewhite' for col in np.arange(1)],\n",
    "    \n",
    "    ax=ax1,\n",
    "    **tablecenter_k)\n",
    "\n",
    "def table_format(a_table, ax, size=12):\n",
    "    table_fonts(a_table, size=size)\n",
    "    make_table_grids(ax)\n",
    "    ax.tick_params(**tabtickp_k)\n",
    "\n",
    "table_format(a_table, ax1) \n",
    "\n",
    "# add table to ax\n",
    "ax1.add_table(a_table )\n",
    "\n",
    "# scatter plot\n",
    "ax2 = fig.add_subplot(gs[0:5])\n",
    "sns.scatterplot(data=allsurveys[~allsurveys.location.isin(bassin_locs)], x='date',  y='pcs_m', alpha=0.5, label=\"tous les autres prélèvements\", color='darkslategray', edgecolor='darkslategray', linewidth=.1,s=70,ax=ax2)\n",
    "sns.scatterplot(data=allsurveys[allsurveys.location.isin(bassin_locs)], x='date',  y='pcs_m', alpha=0.5, label=bassin_label, color='dodgerblue', edgecolor='dodgerblue', linewidth=.1,s=70,ax=ax2)\n",
    "sns.scatterplot(data=lavey, x='date',  y='pcs_m', alpha=1, label=coi, color='red', edgecolor='red', linewidth=1,s=100, ax=ax2)\n",
    "# format scatter\n",
    "ax2.xaxis.set_major_formatter(mths_fmt)\n",
    "ax2.xaxis.set_major_locator(allmonths)\n",
    "ax2.set_xlabel(\"\")\n",
    "ha='right', \n",
    "# context\n",
    "if is_french:\n",
    "    ax1.set_title(F\"{bassin_label}: valeurs clés\", loc='right', ha='right', pad=10)\n",
    "    ax2.set_title(F\"Figure {figure_num}: {french_srs} {startyearmonth} - {endyearmonth}, lacs={lakecount}, rivières={rivercount}.\", loc='left', pad=10)\n",
    "    ax2.set_ylabel(F\"{french_pcm}\", **ylab_k)\n",
    "else:\n",
    "    ax1.set_title(F\"{bassin_label}\", loc='left', pad=10)\n",
    "    ax2.set_ylabel(\"Pieces per meter\", **ylab_k)\n",
    "    ax2.set_title(F\"Figure {figure_num}: survey totals {startyearmonth} - {endyearmonth}, lakes={lakecount}, rivers={rivercount}\", loc='left', pad=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# tag the output:\n",
    "figname = F\"figure{figure_num}.jpg\"\n",
    "atype = \"figure\"\n",
    "tag =  'all surveys: scatter plot, key values table'\n",
    "add_output(figname, tag, fignum=figure_num, atype=atype)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(4,8))\n",
    "figure_num += 1\n",
    "\n",
    "# the context matters for the row and column labels\n",
    "# count the number of samples for this summary\n",
    "n_samples = F\"Nombre de recensements: {lavey.loc_date.nunique()}\"\n",
    "\n",
    "if is_french:\n",
    "    rowLabels = summary_row_fr\n",
    "    col_label = [french_pcm]\n",
    "    title = F\"{coi}, valeurs clés\"\n",
    "else:\n",
    "    col_label = ['pieces per meter']\n",
    "    title = F\"{coi}, key values\"\n",
    "\n",
    "# define the table\n",
    "a_table = mpl.table.table(\n",
    "    cellText=lavey_table,\n",
    "    rowLabels=rowLabels,\n",
    "    rowColours=['antiquewhite' for i in rowLabels],\n",
    "    colLabels=col_label,\n",
    "    colColours=['antiquewhite' for col in np.arange(1)],\n",
    "    \n",
    "    ax=ax1,\n",
    "    **tablecenter_k)\n",
    "\n",
    "def table_format(a_table, ax, size=12):\n",
    "    table_fonts(a_table, size=size)\n",
    "    make_table_grids(ax)\n",
    "    ax.tick_params(**tabtickp_k)\n",
    "\n",
    "table_format(a_table, ax1)\n",
    "ax1.set_title(F\"Figure {figure_num}: {title}\", pad=10, ha='right', loc='right')\n",
    "ax1.set_xlabel(n_samples, **ylab_k)\n",
    "\n",
    "# add table to ax\n",
    "ax1.add_table(a_table)\n",
    "plt.tight_layout()\n",
    "\n",
    "# tag the output:\n",
    "figname = F\"figure{figure_num}.jpg\"\n",
    "atype = \"figure\"\n",
    "tag =  F\"key values {coi}\"\n",
    "add_output(figname, tag, fignum=figure_num, atype=atype)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_french:\n",
    "    sommarkdown = \"\"\"#### <span style=\"color:#008891\"> Les objets les plus communs: toutes les enquêtes </span>\\n    \n",
    "    \"\"\"\n",
    "else:\n",
    "    sommarkdown = \"\"\"#### <span style=\"color:#008891\">Most common objects: all surveys</span>\\n    \n",
    "     \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(sommarkdown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the total quantity and pieces per meter for each code\n",
    "# code quantity:\n",
    "allcodevals = pd.DataFrame(newdfx.groupby('code').quantity.sum())\n",
    "bassincodevals = pd.DataFrame(newdfx[newdfx.location.isin(bassin_locs)].groupby('code').quantity.sum())\n",
    "lavey_codevals = pd.DataFrame(newdfx[(newdfx.location.isin(lavey_locs))].groupby('code').quantity.sum())\n",
    "\n",
    "# code median pcs/m\n",
    "bassin_pcsm = newdfx[(newdfx.location.isin(bassin_locs))].groupby('code').pcs_m.median()\n",
    "lavey_pcsm = newdfx[(newdfx.location.isin(lavey_locs))].groupby('code').pcs_m.median()\n",
    "all_pcsm = newdfx.groupby('code').pcs_m.median()\n",
    "\n",
    "# make a df of all codes for each regional level add description, material, group and percent of regional total:\n",
    "for a_df in [allcodevals, lavey_codevals, bassincodevals]:\n",
    "    # add description and material from the codes df\n",
    "    a_df['description'] = a_df.index.map(lambda x: dfCodes.loc[x].description)\n",
    "    a_df['material'] = a_df.index.map(lambda x: dfCodes.loc[x].material)\n",
    "    a_df['group'] = a_df.index.map(lambda x: a_group_map[x])\n",
    "    a_total = a_df.quantity.sum()\n",
    "    a_df['p_total'] = a_df.quantity/a_total\n",
    "    a_df['p_total'] = a_df['p_total'].round(2)\n",
    "    a_df.sort_values(by='quantity',ascending=False)\n",
    "    a_df.rename(columns={'p_total':'% of total'}, inplace=True)\n",
    "\n",
    "data_num += 1\n",
    "figname = F\"{project_directory}/bassin_code_totals.csv\"\n",
    "tag = F'{bassin_label}: code totals'\n",
    "atype='data'\n",
    "add_output(figname, tag, fignum=data_num, atype=atype, data=bassincodevals)\n",
    "\n",
    "data_num += 1\n",
    "figname = F\"{project_directory}/bassin_code_pcsm_median.csv\"\n",
    "tag = F'{bassin_label}: code median pcs/m'\n",
    "atype='data'\n",
    "add_output(figname, tag, fignum=figure_num, atype=atype, data=bassin_pcsm)\n",
    "\n",
    "# make a top ten table for each region:\n",
    "lavey_tabledata = lavey_codevals[lavey_codevals.quantity > 0][['description', 'material', 'quantity', '% of total', 'group']].copy()\n",
    "top_ten_lavey = lavey_tabledata.sort_values(by='quantity', ascending=False).iloc[:10].copy()\n",
    "top_ten_lavey['pcs_m'] = top_ten_lavey.index.map(lambda x: lavey_pcsm.loc[x])\n",
    "top_ten_lavey_table = top_ten_lavey[['description', 'material', 'quantity','% of total',  'pcs_m', 'group']].copy()\n",
    "top_ten_lavey_table.reset_index(inplace=True)\n",
    "\n",
    "bassin_tabledata = bassincodevals[bassincodevals.quantity > 0][['description', 'material', 'quantity', '% of total', 'group']].copy()\n",
    "top_ten_bassin = bassin_tabledata.sort_values(by='quantity', ascending=False).iloc[:10].copy()\n",
    "top_ten_bassin['pcs_m'] = top_ten_bassin.index.map(lambda x: bassin_pcsm.loc[x])\n",
    "top_ten_bassin_table = top_ten_bassin[['description', 'material', 'quantity','% of total',  'pcs_m', 'group']].copy()\n",
    "top_ten_bassin_table.reset_index(inplace=True)\n",
    "\n",
    "all_tabledata = allcodevals[allcodevals.quantity > 0][['description', 'material', 'quantity', '% of total', 'group']].copy()\n",
    "top_ten_all = all_tabledata.sort_values(by='quantity', ascending=False).iloc[:10].copy()\n",
    "top_ten_all['pcs_m'] = top_ten_all.index.map(lambda x: all_pcsm.loc[x])\n",
    "top_ten_all_table = top_ten_all[['description', 'material', 'quantity','% of total',  'pcs_m', 'group']].copy()\n",
    "top_ten_all_table.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the group total and percentage of total for each code group and level:\n",
    "lavey_groups = useThis[(useThis.location.isin(lavey_locs))].groupby('groupname', as_index=False).quantity.sum()\n",
    "lavey_group_pcsm = useThis[(useThis.location.isin(lavey_locs))].groupby(['groupname','loc_date', 'date', 'location'], as_index='groupname').pcs_m.sum()\n",
    "lavey_group_pcsm = lavey_group_pcsm.groupby('groupname').median()\n",
    "atotal = lavey_groups.quantity.sum()\n",
    "lavey_groups['p_total'] = (lavey_groups.quantity/atotal)*100\n",
    "lavey_groups['p_total'] = lavey_groups['p_total'].round(2)\n",
    "lavey_groups.sort_values(by='p_total', ascending=True, inplace=True)\n",
    "anorder = lavey_groups.groupname.values\n",
    "\n",
    "bassin_groups = useThis[(useThis.location.isin(bassin_locs))].groupby('groupname', as_index=False).quantity.sum()\n",
    "atotal = bassin_groups.quantity.sum()\n",
    "bassin_groups['p_total'] = (bassin_groups.quantity/atotal)*100\n",
    "bassin_groups['p_total'] = bassin_groups['p_total'].round(2)\n",
    "bassin_groups.sort_values(by='p_total', ascending=True, inplace=True)\n",
    "\n",
    "all_groups = useThis[(useThis.location.isin(lavey_locs))].groupby('groupname', as_index=False).quantity.sum()\n",
    "atotal = all_groups.quantity.sum()\n",
    "all_groups['p_total'] = (all_groups.quantity/atotal)*100\n",
    "all_groups['p_total'] = all_groups['p_total'].round(2)\n",
    "all_groups.sort_values(by='p_total', ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = useThis.groupby(['loc_date', 'date','location','water_name', 'groupname'], as_index=False).pcs_m.sum()\n",
    "boxes_l = useThis[useThis.location.isin(lavey_locs)].groupby(['loc_date', 'date','location','water_name', 'groupname'], as_index=False).pcs_m.sum()\n",
    "boxes_b = useThis[useThis.location.isin(bassin_locs)].groupby(['loc_date', 'date','location','water_name', 'groupname'], as_index=False).pcs_m.sum()\n",
    "\n",
    "get_an_order_l = boxes_l.groupby('groupname').pcs_m.median()\n",
    "an_order_of_boxes_l = get_an_order_l.sort_values(ascending=True)\n",
    "a_box_order_l = an_order_of_boxes_l.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the significant value for each code group\n",
    "sig_vals = boxes.groupby('groupname').pcs_m.quantile(sig)\n",
    "sig_vals_b = boxes_b.groupby('groupname').pcs_m.quantile(sig)\n",
    "# get the significant value for each code\n",
    "# sig_vals_code = useThis.groupby('code').pcs_m.median()\n",
    "# sig_vals_bassin  = useThis[useThis.location.isin(bassin_locs)].groupby('code').pcs_m.median()\n",
    "# bassin_pcsm = newdfx[(newdfx.location.isin(bassin_locs))].groupby('code').pcs_m.median()\n",
    "\n",
    "\n",
    "# map the significant value to the results\n",
    "boxes['limit'] = boxes.groupname.map(lambda x: sig_vals[x])\n",
    "\n",
    "# create a boolean for significant\n",
    "boxes['significant'] = boxes.pcs_m >= boxes.limit\n",
    "\n",
    "# map to number of samples, and significant values\n",
    "survey_totals = useThis.groupby(['loc_date','location','water_name', 'date','population'], as_index=False).agg({\"pcs_m\":\"sum\", \"quantity\":\"sum\"})\n",
    "survey_totals.reset_index(inplace=True)\n",
    "\n",
    "# number of samples per lake\n",
    "tries = survey_totals.groupby(['water_name']).loc_date.nunique()\n",
    "\n",
    "# number of locations per lake\n",
    "num_locations = survey_totals.groupby('water_name').location.nunique()\n",
    "\n",
    "# fails: number of locations where object group has been identified\n",
    "num_with = useThis[useThis.quantity > 0].groupby(['water_name', 'groupname']).location.nunique()\n",
    "\n",
    "# fails: number of samples with the object group\n",
    "samps_with = useThis[useThis.quantity > 0].groupby(['water_name', 'groupname']).loc_date.nunique()\n",
    "\n",
    "# median pcs_m\n",
    "median_pcs = boxes.groupby(['water_name', 'groupname']).pcs_m.median()\n",
    "\n",
    "# mean pcs_m\n",
    "mean_pcs = boxes.groupby(['water_name', 'groupname']).pcs_m.mean()\n",
    "\n",
    "# significant values\n",
    "# determine wether or not the event was greater than the 90th percentile\n",
    "\n",
    "# map limit to data\n",
    "boxes['limit'] = boxes.groupname.map(lambda x: sig_vals[x])\n",
    "\n",
    "# create boolean\n",
    "boxes['significant'] = boxes.pcs_m >= boxes.limit\n",
    "\n",
    "# make a df of tests and test failures\n",
    "fails = boxes.groupby(['water_name', 'groupname'], as_index=False).significant.sum()\n",
    "\n",
    "# get the number of samples for the lake\n",
    "fails['samples'] = fails.water_name.map(lambda x: tries[x])\n",
    "\n",
    "# display the ratio of significant values to samples\n",
    "fails['frequency_s'] = fails.significant.astype('str') + '/' + fails.samples.astype('str')\n",
    "\n",
    "# the number of locations\n",
    "fails['locations'] = fails.water_name.map(lambda x: num_locations[x])\n",
    "\n",
    "def locations_with(x,y,somdata):\n",
    "    try:\n",
    "        has = somdata[x][y]        \n",
    "    except:\n",
    "        has = 0\n",
    "    return has\n",
    "        \n",
    "# the number of locations where the object group has been identified\n",
    "fails['loc_with'] = fails.apply(lambda x:locations_with(x['water_name'], x['groupname'], num_with), axis=1)\n",
    "\n",
    "# the number of samples where the object group has been identified\n",
    "fails['samp_with'] = fails.apply(lambda x:locations_with(x['water_name'], x['groupname'], samps_with), axis=1)\n",
    "\n",
    "# samples frequency of failure\n",
    "fails['frequency'] = fails.samp_with.astype('str') + '/' + fails.samples.astype('str')\n",
    "\n",
    "# locations frequency of failure\n",
    "fails['frequency_l'] = fails.loc_with.astype('str') + '/' + fails.locations.astype('str')\n",
    "\n",
    "# median/mean pcs_m:\n",
    "fails['median pcs/m'] = fails.apply(lambda x:locations_with(x['water_name'], x['groupname'], median_pcs), axis=1)\n",
    "fails['mean pcs/m'] = fails.apply(lambda x:locations_with(x['water_name'], x['groupname'], mean_pcs), axis=1)\n",
    "\n",
    "# likelihood\n",
    "fails['likelihood'] = ((fails.loc_with/fails.locations)*(fails.samp_with/(fails.samples+1)))\n",
    "\n",
    "def make_minimum_likelihood(x,samples=100):\n",
    "    # even if an item has never been found\n",
    "    # it is assumed that there is always a chance\n",
    "    # that it may be found. That chance is equal\n",
    "    # 1/n samples * 1/n location (found at one sample and one location)\n",
    "    if x == 0 :\n",
    "            min_val = (1/samples)\n",
    "    else:\n",
    "        min_val = x\n",
    "    return min_val\n",
    "\n",
    "\n",
    "fails['likelihood'] = fails.apply(lambda x:make_minimum_likelihood(x['likelihood']), axis=1)\n",
    "\n",
    "table_data = fails[['water_name','frequency_l', 'frequency',  'frequency_s', 'likelihood','median pcs/m', 'groupname']].copy()\n",
    "table_data.rename(columns={'water_name':'name', 'frequency_l':\"# locations\", 'frequency':\"# samples\", 'frequency_s':'# significant'}, inplace=True)\n",
    "\n",
    "national_median = boxes.groupby('groupname').pcs_m.median()\n",
    "\n",
    "this_data = pd.DataFrame(boxes_l.groupby('groupname').pcs_m.median())\n",
    "this_data['catchment area'] = this_data.index.map(lambda x:lavey_group_pcsm[x] )\n",
    "if is_french:\n",
    "    this_data.rename(columns={'catchment area':'bassin versant', 'pcs_m':'résultats locaux'}, inplace=True)\n",
    "    this_data['groupname'] = this_data.index.map(french_names)\n",
    "    this_data.set_index('groupname', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sum the % of total of the most common objects\n",
    "ptotal = top_ten_bassin_table['% of total'].sum()*100\n",
    "psum = top_ten_bassin_table.quantity.sum()\n",
    "toptenqty = bassincodevals.quantity.sum()\n",
    "\n",
    "# context print tables\n",
    "top_ten_table_p = top_ten_bassin_table.copy()\n",
    "top_ten_table_p['% of total'] = top_ten_table_p['% of total']*100\n",
    "top_ten_table_p['% of total'] = top_ten_table_p['% of total'].round(1).astype('str')\n",
    "top_ten_table_p['% of total'] = top_ten_table_p['% of total'] + \"%\"\n",
    "# context is_french \n",
    "if is_french:\n",
    "    bassin_tabledatacopy = top_ten_table_p.copy()\n",
    "    bassin_tabledatacopy.reset_index(inplace=True)\n",
    "    bassin_tabledatacopy['description'] = bassin_tabledatacopy.code.map(lambda x: count_k(fr_defs_codes[x], limit))\n",
    "    bassin_tabledatacopy['group'] = bassin_tabledatacopy.group.map(lambda x: count_k(french_names[x], limit))\n",
    "    bassin_tabledatacopy.rename(columns=french_columns, inplace=True)\n",
    "    thetabledata = bassin_tabledatacopy[['code','description', 'matériel', 'quantité', '% du total','groupe']].copy()\n",
    "else:\n",
    "    the_top_ten_table = top_ten_table_p[['code','description', 'material', 'quantity', '% of total','group']].copy()\n",
    "    the_top_ten_table.reset_index=(True)\n",
    "    thetabledata = the_top_ten_table.copy()\n",
    "    \n",
    "\n",
    "# make adjustments to table kwargs:\n",
    "tablecenter_k = dict(loc=\"center\", bbox=(0,0,1,1), colWidths=[9, 42, 8, 8, 9,13], cellLoc='center')\n",
    "\n",
    "# plot the table:\n",
    "fig, ax = plt.subplots(figsize=(15, len(thetabledata)*.75))\n",
    "figure_num += 1\n",
    "ax = make_table_grids(ax)\n",
    "a_table = mpl.table.table(\n",
    "    cellText=thetabledata.values,\n",
    "    colLabels=thetabledata.columns,\n",
    "    colColours=['antiquewhite' for col in list(thetabledata.columns)],    \n",
    "    ax=ax,\n",
    "    **tablecenter_k)\n",
    "\n",
    "# set parameters\n",
    "table_fonts(a_table, size=12)\n",
    "ax.tick_params(**tabtickp_k)\n",
    "\n",
    "# add the table\n",
    "ax.add_table(a_table)\n",
    "\n",
    "if is_french:\n",
    "    ax.set_title(F\"Figure {figure_num}: top {len(thetabledata)} codes {bassin_label}, {np.round(ptotal, 2)}% de {'{:,}'.format(toptenqty)} objets\", **title_k14)\n",
    "else:\n",
    "    ax.set_title(F\"Figure {figure_num}: top {len(thetabledata)} codes {bassin_label}, {np.round(ptotal, 2)}% of {'{:,}'.format(toptenqty)} objects\", **title_k14)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "figname = F\"figure{figure_num}.jpg\"\n",
    "tag = F'{bassin_label}: 10 most common objects table'\n",
    "add_output(figname, tag, fignum=figure_num)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a table of results that compares the pcs/m for the national top ten codes accross the regional levels\n",
    "\n",
    "top_ten_bassin_table[coi] = top_ten_bassin_table.code.map(lambda x: lavey_pcsm[x])\n",
    "top_ten_bassin_table['tous les bassins versants'] = top_ten_bassin_table.code.map(lambda x: all_pcsm[x])\n",
    "top_ten_bassin_table.set_index('code', inplace=True)\n",
    "\n",
    "\n",
    "top_ten_tablex = top_ten_bassin_table.rename(columns={'pcs_m':bassin_label})\n",
    "top_ten_tablex['description'] = top_ten_bassin_table.index.map(lambda x: fr_defs_codes[x])\n",
    "top_ten_tablex = top_ten_tablex[[coi, bassin_label,'tous les bassins versants', 'description']].copy()\n",
    "\n",
    "top_ten_tablex.set_index('description', inplace=True)\n",
    "top_ten_tablex.columns = [coi, bassin_label, 'tous les bassins versants']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(17,8))\n",
    "figure_num += 1\n",
    "title=F\"Figure {figure_num}: {bassin_label} les top-ten objets pièces par mètre\"\n",
    "sns.heatmap(data=top_ten_tablex, cmap='YlOrRd', linewidth=0.1, linecolor='darkred', annot=True, ax=ax, annot_kws={\"fontsize\":14})\n",
    "ax.set_ylabel(\"\")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.xaxis.set_ticks_position('top')\n",
    "ax.tick_params(axis='x', which='both',labelsize=14)\n",
    "yticklabs = ax.get_yticklabels()\n",
    "ax.set_yticklabels(yticklabs, fontsize=12, rotation=0)\n",
    "ax.set_title(title, **title_k14)\n",
    "\n",
    "plt.tight_layout()\n",
    "figname = F\"figure{figure_num}.jpg\"\n",
    "tag = 'top-ten regional-local-national median'\n",
    "add_output(figname, tag, fignum=figure_num)\n",
    "plt.show()\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the objects from the coi that exceed the median for catchment area:\n",
    "exceeds_catchment_median = [x for x in lavey_pcsm.index if lavey_pcsm[x] > bassin_pcsm[x]]\n",
    "\n",
    "# remove the objects that were already listed in the top ten\n",
    "exceeds_catchment_median = [x for x in exceeds_catchment_median if x not in top_ten_bassin_table.index]\n",
    "\n",
    "# apply that to the bassin table data:\n",
    "codes_of_interest = bassin_tabledata.loc[bassin_tabledata.index.isin(exceeds_catchment_median)].copy()\n",
    "\n",
    "# make a column for the coi results, map those\n",
    "codes_of_interest[coi] = codes_of_interest.index.map(lambda x: lavey_pcsm.loc[x])\n",
    "\n",
    "# make a column for the bassin results, map that\n",
    "codes_of_interest[bassin_label] = codes_of_interest.index.map(lambda x: bassin_pcsm.loc[x])\n",
    "\n",
    "# get the code description for each object\n",
    "codes_of_interest['description'] = codes_of_interest.index.map(lambda x: fr_defs_codes[x])\n",
    "\n",
    "# sort for charting\n",
    "codes_of_interest = codes_of_interest[[coi, bassin_label, 'description']].copy()\n",
    "codes_of_interest.sort_values(by=coi, ascending=False, inplace=True)\n",
    "codes_of_interest.set_index('description', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "codes_of_interest.columns = [coi, bassin_label]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13,len(codes_of_interest)*.75))\n",
    "figure_num += 1\n",
    "title=F\"Figure {figure_num}: {coi} objets d'intérêt, supérieure à la médiane de {bassin_label}\"\n",
    "sns.heatmap(data=codes_of_interest, cmap='YlOrRd', linewidth=0.1, linecolor='darkred', annot=True, ax=ax, annot_kws={\"fontsize\":14})\n",
    "ax.set_ylabel(\"\")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.xaxis.set_ticks_position('top')\n",
    "ax.tick_params(axis='x', which='both',labelsize=14)\n",
    "yticklabs = ax.get_yticklabels()\n",
    "ax.set_yticklabels(yticklabs, fontsize=14, rotation=0)\n",
    "ax.set_title(title, ha='right', loc='right', fontsize=14, pad=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "figname = F\"figure{figure_num}.jpg\"\n",
    "tag = 'objects of interest'\n",
    "add_output(figname, tag, fignum=figure_num)\n",
    "plt.show()\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "boxes_b['level'] = boxes_b.location.map(lambda x: assign_a_level(x, lavey_locs, [coi, bassin_label]))\n",
    "\n",
    "# assign a regional level to each set of data\n",
    "bassin_groups['level'] = bassin_label\n",
    "lavey_groups['level'] = coi\n",
    "\n",
    "# merge the data sets\n",
    "these_groups = lavey_groups.append(bassin_groups)\n",
    "\n",
    "a_form = mtick.FormatStrFormatter('%.0f%%')\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(14, 10))\n",
    "an_order = [coi, bassin_label]\n",
    "figure_num += 1\n",
    "if is_french:\n",
    "    these_groups['groupname'] = these_groups['groupname'].map(lambda x: french_names[x])\n",
    "    boxes_b['groupname'] = boxes_b['groupname'].map(lambda x: french_names[x])\n",
    "#     this_data.rename(columns={'catchment area':'bassin versant', 'pcs_m':'résultats locaux'}, inplace=True)\n",
    "#     this_data['groupname'] = this_data.index.map(french_names)\n",
    "#     this_data.set_index('groupname', inplace=True)\n",
    "\n",
    "    sns.barplot(data=these_groups, x='groupname', hue='level', hue_order=an_order, palette={coi:'salmon', bassin_label:'dodgerblue'}, y='p_total', ax=ax[0])\n",
    "    ax[0].set_ylabel(F\"{french_pct} niveau régional\", **ylab_k)\n",
    "    ax[0].set_title(F\"groupe de codes % du total, pour chaque niveau régional\", **title_k)\n",
    "    ax[0].set_xlabel(\"\")\n",
    "    ax[0].tick_params(axis='x', which='both',labelsize=12, rotation=90)\n",
    "    ax[0].legend(loc='upper left')    \n",
    "    \n",
    "    sns.boxplot(data=boxes_b[['groupname','pcs_m', 'level']], x='groupname', y='pcs_m', hue='level',  hue_order=an_order, palette={coi:'salmon', bassin_label:'dodgerblue'}, ax=ax[1],  showfliers=False)\n",
    "    ax[1].set_ylabel(F\"{french_pcm}\", **ylab_k)\n",
    "    ax[1].set_title(F\"distribution {french_bg}, {french_nooutliers}\", **title_k)\n",
    "    ax[1].set_xlabel(\"\")\n",
    "    ax[1].tick_params(axis='x', which='both',labelsize=12, rotation=90)\n",
    "\n",
    "else:\n",
    "    sns.barplot(data=these_groups, x='groupname', hue='level', palette=['red', 'blue'], y='p_total', dodge=False, ax=ax[0])\n",
    "    ax[0].set_ylabel(\"Percent total of all objects at regaional level\", **ylab_k)\n",
    "    ax[0].set_title(F\"code group % of total, for each regional level\", **title_k)\n",
    "    ax[0].yaxis.set_major_formatter(a_form)\n",
    "    ax[0].set_xlabel(\"\")\n",
    "    ax[0].tick_params(axis='x', which='both',labelsize=12, rotation=90)\n",
    "    ax[0].legend(loc='upper left')    \n",
    "    \n",
    "    sns.boxplot(data=boxes_l[['groupname','pcs_m']], x='groupname', y='pcs_m', hue='groupname', palette=grouppalette, order=a_box_order_l, dodge=False, ax=ax[1], showfliers=False)\n",
    "    ax[1].set_ylabel(\"Pieces per meter\", labelpad=10, **ylab_k)\n",
    "    ax[1].set_title(F\"disribution of groups, outliers not shown\", **title_k)\n",
    "    ax[1].set_xlabel(\"\")\n",
    "    ax[1].tick_params(axis='x', which='both',labelsize=12, rotation=90)\n",
    "    \n",
    "    \n",
    "    \n",
    "fignum=figure_num\n",
    "\n",
    "if is_french:\n",
    "    suptitle =F\"Figure {figure_num} résultats regroupés par activité ou utilisation : % du total et répartition en pcs/m\"\n",
    "else:\n",
    "    suptitle = F\"Figure {figure_num} results grouped by activity or use: % of total and distribution in pcs/m\"\n",
    "\n",
    "plt.suptitle(suptitle, x=0, y=.99, ha='left', fontsize=16, linespacing=2)\n",
    "\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=.88)\n",
    "figname = F\"figure{figure_num}.jpg\"\n",
    "atype='figure'\n",
    "tag = 'code groups: percentage of total, distribution, regional results'\n",
    "add_output(figname, tag, fignum=fignum, atype=atype)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = boxes_b.groupby(['level', 'groupname'], as_index=False).pcs_m.median()\n",
    "a=a.pivot(columns='level', index='groupname')\n",
    "a.columns = a.columns.get_level_values(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9,10))\n",
    "figure_num += 1\n",
    "if is_french:\n",
    "    title = F\"Figure {figure_num} groupes de codes: pièces médianes par mètre\"\n",
    "    \n",
    "else:\n",
    "    title = \"Local median compared to catchment area\"\n",
    "    \n",
    "sns.heatmap(data=a, cmap='YlOrRd', linewidth=0.1, linecolor='darkred', annot=True, annot_kws={\"fontsize\":14}, ax=ax)\n",
    "ax.set_ylabel(\"\")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.xaxis.set_ticks_position('top')\n",
    "ax.tick_params(axis='x', which='both',labelsize=12)\n",
    "yticklabs = ax.get_yticklabels()\n",
    "ax.set_yticklabels(yticklabs, fontsize=14, rotation=0)\n",
    "ax.set_title(title, **title_k14)\n",
    "plt.tight_layout()\n",
    "figname = F\"figure{figure_num}.jpg\"\n",
    "atype='figure'\n",
    "tag = F\"code groups: {coi} and {bassin_label} median pcs/m\"\n",
    "add_output(figname, tag, fignum=figure_num, atype=atype)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_tabledata = lavey_tabledata.reset_index()\n",
    "\n",
    "# context print tables\n",
    "this_tabledata_p = lavey_tabledata.copy()\n",
    "a_total = this_tabledata.quantity.sum()\n",
    "this_tabledata_p['% of total'] = this_tabledata_p['% of total']*100\n",
    "this_tabledata_p['% of total'] = this_tabledata_p['% of total'].round(1).astype('str')\n",
    "this_tabledata_p['% of total'] = this_tabledata_p['% of total'] + \"%\"\n",
    "# context is_french \n",
    "if is_french:\n",
    "    tabledatacopy = this_tabledata_p.copy()\n",
    "    tabledatacopy.reset_index(inplace=True)\n",
    "    tabledatacopy.sort_values(by='group', inplace=True)\n",
    "    tabledatacopy['description'] = tabledatacopy.code.map(lambda x: count_k(fr_defs_codes[x], limit))\n",
    "    tabledatacopy['group'] = tabledatacopy.group.map(lambda x: count_k(french_names[x], limit))\n",
    "    tabledatacopy.rename(columns=french_columns, inplace=True)\n",
    "    thetabledata = tabledatacopy[['code','description', 'matériel', 'quantité', '% du total','groupe']].copy()\n",
    "    this_tabledata = thetabledata \n",
    "tablecenter_k = dict(loc=\"center\", bbox=(0,0,1,1), colWidths=[9, 44, 8, 8, 9,11], cellLoc='center')\n",
    "\n",
    "# plot the table:\n",
    "print(len(this_tabledata.columns))\n",
    "fig, ax = plt.subplots(figsize=(18, len(this_tabledata)*.75))\n",
    "figure_num += 1\n",
    "ax = make_table_grids(ax)\n",
    "a_table = mpl.table.table(\n",
    "    cellText=this_tabledata.values,\n",
    "    colLabels=this_tabledata.columns,\n",
    "    colColours=['antiquewhite' for col in list(this_tabledata.columns)],    \n",
    "    ax=ax,\n",
    "    **tablecenter_k)\n",
    "\n",
    "# set parameters\n",
    "table_fonts(a_table, size=12)\n",
    "ax.tick_params(**tabtickp_k)\n",
    "\n",
    "# add the table\n",
    "ax.add_table(a_table)\n",
    "\n",
    "\n",
    "if is_french:\n",
    "    ax.set_title(F\"Figure {figure_num}: {coi} tous {a_total} objets trouvés\", **title_k14)\n",
    "else:\n",
    "    ax.set_title(F\"Figure {figure_num}: {coi} all objects found\", **title_k14)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "figname = F\"figure{figure_num}.jpg\"\n",
    "tag = F\"{coi}: all objects table\"\n",
    "add_output(figname, tag, fignum=figure_num)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map number of sample per locations\n",
    "tries_l = boxes.groupby('location').loc_date.nunique()\n",
    "\n",
    "# map number of times at least one item was found\n",
    "fails_l = boxes[boxes.pcs_m > 0].groupby(['location', 'groupname']).loc_date.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a df to track codegroup fail rates for each location\n",
    "fails_beach = boxes.groupby(['location', 'water_name','groupname'], as_index=False).pcs_m.median()\n",
    "\n",
    "# add a tries and fails column for each location and group\n",
    "fails_beach['tries'] = fails_beach.location.map(lambda x: tries_l[x])\n",
    "fails_beach['fails'] = fails_beach.apply(lambda x:locations_with(x['location'], x['groupname'], fails_l), axis=1)\n",
    "\n",
    "# the ratio samples to failures\n",
    "fails_beach['likelihood'] = (fails_beach.fails/fails_beach.tries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the fail rate per beach (how many times the quantity was greater than zero)\n",
    "fails_beach_bassin  = fails_beach[fails_beach.water_name.isin(bassin)].groupby('groupname').agg({ 'tries':'sum', 'fails':'sum'})\n",
    "fails_beach_bassin['group_rates'] = fails_beach_bassin.fails/ fails_beach_bassin.tries\n",
    "fails_beach_lavey = fails_beach[fails_beach.location.isin(lavey_locs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = boxes.groupby('location').significant.sum()\n",
    "these_beaches['significant'] = these_beaches.index.map(lambda x: a.loc[x])\n",
    "these_beaches['median pcs/m'] = these_beaches.index.map(lambda x: allsurveys_pcs_median[x])\n",
    "these_beaches.to_csv(F\"{project_directory}/these_beaches.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_data = fails_beach[fails_beach.water_name.isin(bassin)].copy()\n",
    "this_data['pop'] = this_data.location.map(lambda x: these_beaches.loc[x]['population'])\n",
    "this_data = this_data.sort_values(by='pop')\n",
    "this_order = this_data.location.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_map_palette = 'YlOrRd'\n",
    "linecolor = 'white'\n",
    "\n",
    "\n",
    "fig,axx = plt.subplots(figsize=(19,12))\n",
    "figure_num += 1\n",
    "\n",
    "a = this_data[['location', 'groupname',  'pcs_m']].copy()\n",
    "\n",
    "if is_french:\n",
    "    a['groupname'] = a.groupname.map(lambda x: french_names[x])\n",
    "    title = F\"Figure {figure_num}: {bassin_label} groupes de codes, pièces par mètre médiane pour chaque lieu.\"\n",
    "else:\n",
    "    title = \"code groups, median pieces per meter for each location.\"\n",
    "a=a.pivot(columns='location', index='groupname')\n",
    "other_locs = [x for x in a.columns.get_level_values(1) if x not in lavey_locs]\n",
    "a = a.reindex(this_order, axis=1, level=1)\n",
    "sns.heatmap(a,\n",
    "            cmap=heat_map_palette, ax=axx, annot=True,annot_kws={'fontsize':12}, linewidths=.5, linecolor=linecolor)\n",
    "labels = [a_text.get_text() for a_text in axx.get_xticklabels()]\n",
    "newlabels = []\n",
    "for i, a_text in enumerate(labels):\n",
    "    oldlabel = a_text\n",
    "    newlabel = oldlabel[6:]\n",
    "    newlabels.append(newlabel)\n",
    "\n",
    "axx.set_xticklabels(newlabels, fontsize=12)\n",
    "axx.set_ylabel(\" \")\n",
    "axx.set_xlabel(\" \")\n",
    "axx.tick_params(axis='both', labelsize=12, pad=15)\n",
    "axx.set_title(F\"{title}\", **title_k14)\n",
    "\n",
    "    \n",
    "plt.tight_layout()\n",
    "atype=\"figure\"\n",
    "figname = F\"figure{figure_num}.jpg\"\n",
    "tag = F\"{bassin_label} all locations median pcs/m\"\n",
    "add_output(figname,tag, fignum=figure_num, atype=atype)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figures and data produced by this notebook:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_df = pd.DataFrame(files_generated)\n",
    "files_df.rename(columns={'tag':'description'}, inplace=True)\n",
    "\n",
    "files_df = files_df[['type','number', 'description']]\n",
    "files_df = files_df.sort_values(by='type')\n",
    "files_df.sort_values(by=['type','number'], inplace=True)\n",
    "tablecenter_k = dict(loc=\"top left\", bbox=(0,0,1,1), cellLoc='center', colWidths=[20,10,70], fontsize=12)\n",
    "tablecenter_kx = dict(loc=\"top left\", bbox=(0,0,1,1), cellLoc='center', fontsize=12)\n",
    "        \n",
    "with sns.axes_style('white', {'xtick.color':'white', 'ytick.color':'white'}):\n",
    "    fig, axs = plt.subplots(figsize=(12,(len(files_df)*.75)), frameon=False)\n",
    "    figure_num += 1\n",
    "    sns.despine(fig=fig, top=True, left=True, right=True, bottom=True)\n",
    "\n",
    "    make_table_grids(ax1)    \n",
    "\n",
    "    a_table = axs.add_table(mpl.table.table(\n",
    "        cellText=files_df.values,\n",
    "        colLabels=files_df.columns,\n",
    "        colColours=['antiquewhite' for col in files_df.columns],\n",
    "        ax=axs,\n",
    "        **tablecenter_k))\n",
    "\n",
    "\n",
    "    table_fonts(a_table)\n",
    "\n",
    "    axs.tick_params(**tabtickp_k)\n",
    "    figname = F\"figure{figure_num}.jpg\"\n",
    "    tag = F\"{coi}: output table\"\n",
    "    add_output(figname, tag, fignum=figure_num)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hopefully that just worked for you\n",
    "\n",
    "if not contact analyst@hammerdirt.ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
