{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please read this if you want the notebook to work\n",
    "\n",
    "\n",
    "__What this does:__ Get the data from the API and perform basic preprocessing and aggregating\n",
    "\n",
    "__When to do it:__ Run this module **weekly** to pick up any surveys that were completed. Then run the other notebooks and scripts off the generated data.\n",
    "\n",
    "#### Tasks\n",
    "\n",
    "1. Retrieve data from the api and save output as .json\\*\n",
    "2. Adds the 'location-date' column to the dimensional data and survey data\n",
    "3. Attaches the water_name variable to the survey data\n",
    "3. Fixes city names that don't match the official name\n",
    "   1. tells which ones it could not fix\n",
    "   2. drops records that could not be fixed\\*\n",
    "4. Attaches BFS number to location data\n",
    "5. Identifies BFS numbers in the beach data that do not match STATPOP or STATENT\n",
    "   1. drops records where no match is found\\*\n",
    "6. Adds population to the beach data (uses the BFS number)\n",
    "7. Adds zeros to the results\\*\n",
    "8. Aggregates on the parent_code variable of the code data\n",
    "9. Saves all that as .csv format and creates a data_directory.json file\n",
    "   1. the json file is loaded in the template\n",
    "\n",
    "   \n",
    "\n",
    "\\* notes:\n",
    "\n",
    "**retrieve data** the endpoints can be changed, the ones given are the root of all the other endpoints\n",
    "\n",
    "**drop records** there are methods available include new city names and BFS numbers. If you introduce a new location it may \n",
    "end up on the list of locations to drop. If that is the case, you can supply the correct information in this workbook and save the data. For STATENT or STATPOP changes see the statent workbook.\n",
    "\n",
    "**Add zeroes** For this study, if an object was found once, anywhere it is considered as part of the national inventory. Therfore if it is not found/indentified on a survey then this is data we need to track. If an object is part of the national inventory and it was not identified in a survey, the object is added to the survey with a quantity value of zero.\n",
    "\n",
    "**The survey dimensions valid only for 2020 project** contain the length, area, weights, number of participants and the like. So if you are doing a surface area calculation or wieghts then this is the data you need. The survey dimensions are  keyed by the tuple (location, date), there is one for each survey.\n",
    "\n",
    "\n",
    "\n",
    "questions or comments: analyst@hammerdirt.ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import utilities.utility_functions as ut\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# the local file structure. The resources are located in the corresponding directory.\n",
    "# the purpose and date should be included in the filename when saving results to output\n",
    "survey_data, location_data, code_defs, stat_ent, geo_data, output = ut.make_local_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#008891\">Get the current data from the server </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_the_data(end_points):\n",
    "    \"\"\"Takes an array of 2d tuples or arrays ('name', 'url') and\n",
    "    returns a dictionary of named data objects.\n",
    "\n",
    "    Used in all notebooks that read or write data.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    for pair in end_points:\n",
    "        print(pair[1])\n",
    "        data[pair[0]] = requests.get(pair[1])\n",
    "    return data\n",
    "\n",
    "\n",
    "def write_the_data(a_source, a_dir):\n",
    "    \"\"\"Writes the response objects (a JSON object) to the provided location.\n",
    "\n",
    "    Used in notebooks that make an api call\n",
    "    \"\"\"\n",
    "    outPut = []\n",
    "    with open(a_dir, 'w') as outfile:\n",
    "        json.dump(a_source.json(), outfile)\n",
    "\n",
    "def put_the_data_to_local(end_points, here):\n",
    "    \"\"\"Gets the data from the provided URL and writes it to the provided location.\n",
    "\n",
    "    Used in notebooks that make an api call\n",
    "    \"\"\"\n",
    "    the_dict = get_the_data(end_points)\n",
    "    write_the_data(the_dict, here)\n",
    "    \n",
    "def unpack_survey_results(survey_results):\n",
    "    \"\"\"Unpacks the surveys-results api-endpoint and adds the location name to each result dict.\n",
    "\n",
    "    Used in notebooks that make an api call to 'https://mwshovel.pythonanywhere.com/api/surveys/daily-totals/code-totals/swiss/'\n",
    "    \"\"\"\n",
    "    unpacked = []\n",
    "    for location_data in survey_results:\n",
    "        location = location_data['location']\n",
    "        for each_dict in location_data['dailyTotals']:\n",
    "            each_dict['location']=location\n",
    "            unpacked.append(each_dict)\n",
    "    return unpacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://mwshovel.pythonanywhere.com/api/surveys/daily-totals/code-totals/swiss/\n",
      "https://mwshovel.pythonanywhere.com/api/surveys/dim-data/dim-data-list/\n",
      "https://mwshovel.pythonanywhere.com/api/list-of-beaches/swiss/\n",
      "https://mwshovel.pythonanywhere.com/api/mlw-codes/list/\n"
     ]
    }
   ],
   "source": [
    "# whatever prefix you are using\n",
    "prefix = 'https://mwshovel.pythonanywhere.com/api/'\n",
    "\n",
    "# the resources that you need from the api:\n",
    "data_i_need = [\n",
    "    'surveys/daily-totals/code-totals/swiss/',\n",
    "    'surveys/dim-data/dim-data-list/',    \n",
    "    'list-of-beaches/swiss/',\n",
    "    'mlw-codes/list/',    \n",
    "   \n",
    "]\n",
    "\n",
    "# some names for the files\n",
    "data_names = [\n",
    "    'daily_code_totals',\n",
    "    'survey_dims',\n",
    "    'list_of_beaches',\n",
    "    'mlw_codes'   \n",
    "]\n",
    "\n",
    "# places to store the data\n",
    "dirs = [\n",
    "    F\"{survey_data}/daily_code_totals.json\",\n",
    "    F\"{survey_data}/survey_dims.json\",\n",
    "    F\"{location_data}/list_of_beaches.json\",\n",
    "    F\"{code_defs}/mlw_codes.json\"    \n",
    "]\n",
    "\n",
    "data_directory = [*dirs]\n",
    "\n",
    "# make tuple from the name and the urls\n",
    "get_this = [(data_names[i],F\"{prefix}{x}\") for i,x in enumerate(data_i_need)]\n",
    "\n",
    "# get the data\n",
    "a_collection_of_objects = get_the_data(get_this)\n",
    "\n",
    "# write the data to local in .json format\n",
    "for i,key in enumerate(data_names):\n",
    "    write_the_data(a_collection_of_objects[key],dirs[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#008891\"> Unpack the data and save to csv </span>\n",
    "\n",
    "#### The api is designed to facilitate web output, so there is some prepocessing to do\n",
    "\n",
    "Before going any further make csv copy of the refreshed data. The survey data needs to be unpacked. The results are grouped by location and then the survey dates and values. To unpack we need to assign the location to each set of values.\n",
    "\n",
    "*Make loc-date variable*. The loc-date variable does not come off the api. This needs to be done here. There are multiple locations with surveys on the same date. To differentiate between them the location and date need to be combined for each survey record. This is done for the dimensional data as well. \n",
    "\n",
    "*Integrate BFS number to beach data*. The BFS number is the key to population and economic data at the municipal level. The BFS number is attached to the beach data by way of the 'city' variable that defines each survey location.\n",
    "\n",
    "*Attach zeros to each survey*. **In this type of data, how many times something was not seen is just as important as how many times it was seen**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_details = ut.json_file_get(dirs[0])\n",
    "survey_dims = ut.json_file_get(dirs[1])\n",
    "beach_data = ut.json_file_get(dirs[2])\n",
    "code_definitions = ut.json_file_get(dirs[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#008891\"> Add loc_date variable to survey and dimensional data </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sd_up = ut.unpack_survey_results(survey_details)\n",
    "sd_df = pd.DataFrame(sd_up)\n",
    "sd_df['loc_date'] = tuple(zip(sd_df.location, sd_df.date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "survey_key          aare-limmatspitz2020-07-13120\n",
       "date                                   2020-07-13\n",
       "length                                        120\n",
       "area                                       253.00\n",
       "mac_plast_w                               155.000\n",
       "mic_plas_w                                  0.033\n",
       "total_w                                     0.155\n",
       "est_weight                                    0.0\n",
       "num_parts_staff                                 1\n",
       "num_parts_other                                 0\n",
       "time_minutes                                   90\n",
       "participants                               [\"HD\"]\n",
       "project                                      2020\n",
       "is_2020                                      True\n",
       "location                         aare-limmatspitz\n",
       "loc_date           (aare-limmatspitz, 2020-07-13)\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdim_df = pd.DataFrame(survey_dims)\n",
    "sdim_df['loc_date'] = tuple(zip(sdim_df.location, sdim_df.date))\n",
    "sdim_df.iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:blue'> Intgerate bfs number </span>\n",
    "\n",
    "The location data or 'beach data' contains the city of survey location. The city is put in by the surveyor and does not always match what the 'official' city name is.\n",
    "\n",
    "Here the city names are corrected and then the BFS number is mapped to the beach data with the corrected city name.\n",
    "\n",
    "There is economic and demographic data availbale for the municipalities in this study. *The statistique structurelle des entreprises* give estimates on employment in different economic categories.\n",
    "\n",
    "Available at: [STATENT](https://www.bfs.admin.ch/bfs/fr/home/statistiques/industrie-services/entreprises-emplois/structure-economie-entreprises.html)\n",
    "\n",
    "Population data can be found here: [STATPOP](https://www.bfs.admin.ch/bfs/fr/home/statistiques/population/enquetes/statpop.html)\n",
    "\n",
    "All of this data is keyed according to the BFS number for each municipality. The BFS number can be found here: [Répertoire officiel des communes de Suisse](https://www.bfs.admin.ch/bfs/fr/home/bases-statistiques/repertoire-officiel-communes-suisse.html)\n",
    "\n",
    "We are using statent-2017-GMDE and statpop-2018-GMDE in the resources directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# set the index of beach data to slug:\n",
    "beach_data = pd.DataFrame(beach_data).set_index('slug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# set the index of beach data to slug:\n",
    "# beach_data = pd.DataFrame(beach_data).set_index('slug')\n",
    "\n",
    "# get the statent and statpop:\n",
    "stat_ent_c = pd.read_csv(F\"{stat_ent}/stat_ent_corrected.csv\").set_index('GDENR')\n",
    "stat_pop = pd.read_csv(F\"{stat_ent}/STATPOP2018_GMDE.csv\").set_index('GDENR')\n",
    "\n",
    "# get the location-bfs keys:\n",
    "bfs_loc_key = pd.read_csv(F\"{stat_ent}/bfs_num.csv\").set_index('GDENAME')\n",
    "\n",
    "# get the dict of beach names to be fixed\n",
    "# this has the city names we know need to be fixed already\n",
    "re_names = ut.json_file_get(F\"{stat_ent}/not_names.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These city names should be changed at the server\n",
      "['Saint Gingolph', 'St Gallen', 'Jona city', 'Saint Sulpice', 'Illnau Effretikon', 'Zurich', 'biel', 'Cheyres', 'Bremgarten', 'Domat Ems', 'Kradolf Schönenberg', 'Wil', 'Zell', 'Disentis Mustér', 'La Tour de Peilz', 'Küsnacht', 'Hauterive', 'Biel', 'Brienze (BE)', 'Lavey-les-bains']\n",
      "\n",
      "\n",
      "These city names have not been accounted for:\n",
      "['Reinach', 'Huningue', 'Büsingen a Hochrhein', 'Selyshche']\n"
     ]
    }
   ],
   "source": [
    "# find city names in beach data that are not in the official names list:\n",
    "c_names = beach_data.city.unique()\n",
    "gde_names = bfs_loc_key.index\n",
    "\n",
    "\n",
    "# these are the city names that need changing for sure:\n",
    "not_names_keys = list(re_names.keys())\n",
    "\n",
    "check_me = [x for x in c_names if x not in gde_names]\n",
    "print(F\"These city names should be changed at the server\\n{not_names_keys}\\n\")\n",
    "\n",
    "# these are the municipalities that we cannot account for:\n",
    "not_accounted_for = [x for x in check_me if x not in not_names_keys]\n",
    "print(F\"\\nThese city names have not been accounted for:\\n{not_accounted_for}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#008891\"> If needed update the index of misnamed cities and save to local. </span>\n",
    "\n",
    "Then use it to update the city names in beach_data, uncomment the code below and insert a key, value pair to the update statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following citys could not be reconciled with the register of communes, they are being dropped from the wotking data\n",
      "['birs_reinach_dinuccin', 'rhein_basel_blarerp_1', 'rhein_basel_blarerp_2', 'rhein_laag_jungbluthn', 'untersee_steckborn_siedlerm']\n",
      "\n",
      "The original data can always be used by refering to .JSON object in the surveydata folder.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# update the renames dictionary\n",
    "# re_names.update({\n",
    "#    \"Lavey-les-bains\":\"Lavey-Morcles\"\n",
    "# })\n",
    "# a_file_name = '{}/{}'.format(my_folders['data'], 'not_names.json')\n",
    "# ut.push_this_to_json(filename=a_file_name, data=re_names)\n",
    "\n",
    "# if the above script needed to be run then there are city names to be fixed\n",
    "# either way the script needs to run to fix the names that we already know about.\n",
    "\n",
    "def fix_city_names(x, the_names):\n",
    "    if x in the_names.keys():\n",
    "        package = the_names[x]\n",
    "    else:\n",
    "        package =x\n",
    "    return package\n",
    "\n",
    "# replace the old names with the new and delete the old ones\n",
    "beach_data['city2']= beach_data.city.map(lambda x: fix_city_names(x, re_names))\n",
    "beach_data.drop('city', axis=1, inplace=True)\n",
    "beach_data.rename(columns={\"city2\":\"city\"}, inplace=True)\n",
    "\n",
    "# see if their are any names that need to be fixed:\n",
    "c_names = beach_data.city.unique()\n",
    "\n",
    "check_me = [x for x in c_names if x not in gde_names]\n",
    "drop_us = beach_data.loc[beach_data.city.isin(check_me)].index.to_list()\n",
    "\n",
    "print(F\"The following citys could not be reconciled with the register of communes, they are being dropped from the wotking data\\n{drop_us}\")\n",
    "print(\"\\nThe original data can always be used by refering to .JSON object in the surveydata folder.\\n\")\n",
    "beach_data = beach_data[~beach_data.city.isin(check_me)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#008891\"> At this point add the bfs number to the beach data </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>bfsnum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slug</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aabach</th>\n",
       "      <td>Aabach</td>\n",
       "      <td>3338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aare-limmatspitz</th>\n",
       "      <td>Aare Limmatspitz</td>\n",
       "      <td>4029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aare-solothurn-lido-strand</th>\n",
       "      <td>Aare Solothurn Lido Strand</td>\n",
       "      <td>2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aarezufluss_bern_scheurerk</th>\n",
       "      <td>aarezufluss_bern_scheurerk</td>\n",
       "      <td>351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aare_bern_caveltin</th>\n",
       "      <td>Aare_Bern_CaveltiN</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              location  bfsnum\n",
       "slug                                                          \n",
       "aabach                                          Aabach    3338\n",
       "aare-limmatspitz                      Aare Limmatspitz    4029\n",
       "aare-solothurn-lido-strand  Aare Solothurn Lido Strand    2601\n",
       "aarezufluss_bern_scheurerk  aarezufluss_bern_scheurerk     351\n",
       "aare_bern_caveltin                  Aare_Bern_CaveltiN     356"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beach_data['bfsnum'] = beach_data.city.map(lambda x: bfs_loc_key.loc[x]['GDENR'])\n",
    "# use that as a map to add the bfs number to the beach data\n",
    "beach_data.iloc[:5][['location', 'bfsnum']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check for nan values after adding the bfs number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beach_data['bfsnum'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#008891\"> Check the bfs num against statent and stapop bfs numbers </span>\n",
    "\n",
    "Some time these lists get out of sync, make sure that all the locations have a bfs number that is valid for statent and/or statpop data.\n",
    "\n",
    "For some locations the city name will not resolve to a BFS number. Those records will be dropped (for now). This is a limited list of locations and none of them are on a lake of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are in statent but not in statpop:\n",
      "[132, 133, 217, 222, 624, 865, 875, 878, 3503, 3521, 3522, 3611, 3616, 4113, 6402, 6409, 6410, 6411, 6414, 6415, 6705, 6803]\n",
      "\n",
      "\n",
      "These are in statpop but not in statent:\n",
      "[294, 295, 3544]\n",
      "\n",
      "These are the pop keys that do not match beach_data keys:\n",
      "[]\n",
      "\n",
      "These are the stat keys that do not match beach_data keys:\n",
      "[295]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "statent_keys = list(stat_ent_c.index)\n",
    "statpop_keys = list(stat_pop.index)\n",
    "\n",
    "ent_pop = [x for x in statent_keys if x not in statpop_keys]\n",
    "pop_ent = [x for x in statpop_keys if x not in statent_keys]\n",
    "keysweneed = beach_data.bfsnum.unique()\n",
    "\n",
    "missing_pop_keys = [x for x in keysweneed if x not in statpop_keys]\n",
    "missing_ent_keys = [x for x in keysweneed if x not in statent_keys]\n",
    "\n",
    "print(F\"These are in statent but not in statpop:\\n{ent_pop}\\n\")\n",
    "print(F\"\\nThese are in statpop but not in statent:\\n{pop_ent}\\n\")\n",
    "print(F\"These are the pop keys that do not match beach_data keys:\\n{missing_pop_keys}\\n\")\n",
    "print(F\"These are the stat keys that do not match beach_data keys:\\n{missing_ent_keys}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#008891\"> Chase up any bfs numbers that don't match. </span>\n",
    "\n",
    "As of january 2021 there is one number: 295 that remains unacounted for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sihl_horgen_bucherf\n"
     ]
    }
   ],
   "source": [
    "print(beach_data[beach_data.bfsnum == 295].index.values[0])\n",
    "drop_us.append(beach_data[beach_data.bfsnum == 295].index.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BFS num was appended to all records except for the follwoing:\n",
      "['birs_reinach_dinuccin', 'rhein_basel_blarerp_1', 'rhein_basel_blarerp_2', 'rhein_laag_jungbluthn', 'untersee_steckborn_siedlerm', 'sihl_horgen_bucherf']\n",
      "They have been dropped from the data\n"
     ]
    }
   ],
   "source": [
    "# drop that record from the working data\n",
    "# add that name to the check me list\n",
    "\n",
    "beach_data = beach_data[beach_data.bfsnum != 295].copy()\n",
    "print(F\"The BFS num was appended to all records except for the follwoing:\\n{drop_us}\\nThey have been dropped from the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#008891\"> Add population to beach data </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>population</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slug</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aabach</th>\n",
       "      <td>Aabach</td>\n",
       "      <td>3710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aare-limmatspitz</th>\n",
       "      <td>Aare Limmatspitz</td>\n",
       "      <td>5414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aare-solothurn-lido-strand</th>\n",
       "      <td>Aare Solothurn Lido Strand</td>\n",
       "      <td>16777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aarezufluss_bern_scheurerk</th>\n",
       "      <td>aarezufluss_bern_scheurerk</td>\n",
       "      <td>133883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aare_bern_caveltin</th>\n",
       "      <td>Aare_Bern_CaveltiN</td>\n",
       "      <td>13054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              location  population\n",
       "slug                                                              \n",
       "aabach                                          Aabach        3710\n",
       "aare-limmatspitz                      Aare Limmatspitz        5414\n",
       "aare-solothurn-lido-strand  Aare Solothurn Lido Strand       16777\n",
       "aarezufluss_bern_scheurerk  aarezufluss_bern_scheurerk      133883\n",
       "aare_bern_caveltin                  Aare_Bern_CaveltiN       13054"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_pop_keyed = stat_pop['B18BTOT'].copy()\n",
    "\n",
    "beach_data['population'] = beach_data.bfsnum.map(lambda x: tot_pop_keyed[x])\n",
    "beach_data[['location', 'population']].iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#008891\"> Add zeroes to results</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([('baby-plage-geneva', '2021-02-10'),\n",
       "       ('baby-plage-geneva', '2021-01-16'),\n",
       "       ('baby-plage-geneva', '2020-12-16')], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# go back to the original results and drop the records identified previously\n",
    "adf = sd_df[~sd_df.location.isin(drop_us)].copy()\n",
    "\n",
    "# make a list of all the codes that have been identified at least once:\n",
    "current_codes = adf.code.unique()\n",
    "\n",
    "# make sure to add the loc_date identifier:\n",
    "adf['loc_date'] = tuple(zip(adf['location'],adf['date'],))\n",
    "\n",
    "# get a list of surveys using the loc_date column:\n",
    "current_surveys = adf.loc_date.unique()\n",
    "\n",
    "current_surveys[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def insert_zeroes_to_a_group(a_group, current_codes):\n",
    "    g_codes = a_group.code.unique()\n",
    "    g_date = a_group.date.unique()\n",
    "    g_loc = a_group.location.unique()\n",
    "    missing = [x for x in current_codes if x not in g_codes]\n",
    "    if len(missing) > 0:\n",
    "        add_us = []\n",
    "        for code in missing:            \n",
    "            add_us.append({'date':g_date[0], 'location':g_loc[0], 'code':code, 'pcs_m':0, 'quantity':0, 'loc_date':(g_loc[0], g_date[0])})\n",
    "    return a_group.append(add_us)\n",
    "\n",
    "def insert_zeroes_survey_results(adf, current_surveys, current_codes):\n",
    "    new_df = pd.DataFrame(columns=adf.columns)\n",
    "    for the_tup in current_surveys:\n",
    "        a_group = adf.loc[(adf.loc_date == the_tup)]\n",
    "        with_zed = insert_zeroes_to_a_group(a_group, current_codes)\n",
    "        new_df = pd.concat([new_df, with_zed])\n",
    "    return new_df\n",
    "        \n",
    "\n",
    "survey_data_with_zeroes =  insert_zeroes_survey_results(adf, current_surveys, current_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#008891\">Save the survey data and beach data locally as a csv</span>\n",
    "\n",
    "Before saving, add the water_body name to the survey results. This saves a step in aggregating regional data later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "map_water = beach_data.water_name\n",
    "survey_data_with_zeroes['water_name'] = survey_data_with_zeroes['location'].map(lambda x: map_water[x] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "a_file_name = F\"{survey_data}/results_with_zeroes.csv\"\n",
    "data_directory.append(a_file_name)\n",
    "survey_data_with_zeroes.to_csv(a_file_name, index=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "another_file_name = F\"{survey_data}/dims_data.csv\"\n",
    "data_directory.append(another_file_name)\n",
    "sdim_df.to_csv(another_file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "location_file = F\"{location_data}/beaches_pop_bfs.csv\"\n",
    "data_directory.append(location_file)\n",
    "beach_data.to_csv(location_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "code_file = F\"{code_defs}/mlw_codes.csv\"\n",
    "data_directory.append(code_file)\n",
    "code_df = pd.DataFrame(code_definitions)\n",
    "code_df.to_csv(code_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#008891\"> Aggregate parent codes </span>\n",
    "\n",
    "All of the codes have a 'parent_code' attribute. The parent_code is the group of several other codes. For example G74 (Extruded polystyrene) is the parent code to: G909 and G910 (extruded polystyrene < or > 25mm).\n",
    "\n",
    "If you want to aggregate on MLW codes then use the aggregated data. The method to aggregate the results off the API are below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "302165"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can use the data we just finished working with:\n",
    "aggdf = pd.read_csv(F\"{survey_data}/results_with_zeroes.csv\")\n",
    "len(aggdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['code', 'material', 'description', 'source', 'source_two',\n",
      "       'source_three', 'parent_code', 'direct', 'single_use', 'micro',\n",
      "       'ospar_code', 'p_code'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# make a new column called p_code, make that equal to the parent_code column\n",
    "# this works because the default value of parent_code is \"Parent code\", so any code\n",
    "# that does not have a parent code will not aggregate.\n",
    "code_df['p_code'] = code_df['parent_code']\n",
    "\n",
    "# set the the value of p_code = to the value we intend on aggregating\n",
    "# the value of p_code will either be the value of the code itself or the parent_code\n",
    "# check np.wehre to see how this method works, pandas also has an implementation\n",
    "code_df['p_code'] = np.where(code_df['p_code'] == 'Parent code', code_df['code'], code_df['p_code'])\n",
    "print(code_df.columns)\n",
    "code_df.set_index('code', inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting back to code G74, check the parent code of G910\n",
      "material                                                  Plastic\n",
      "description     Foamed-plastic for insulation; yellow, pink, b...\n",
      "source                                               Construction\n",
      "source_two                                Where does it come from\n",
      "source_three                                                 none\n",
      "parent_code                                                   G74\n",
      "direct                                                      False\n",
      "single_use                                                  False\n",
      "micro                                                       False\n",
      "ospar_code                                                     46\n",
      "p_code                                                        G74\n",
      "Name: G910, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(F\"Getting back to code G74, check the parent code of G910\\n{code_df.loc['G910']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loc_date</th>\n",
       "      <th>code</th>\n",
       "      <th>date</th>\n",
       "      <th>pcs_m</th>\n",
       "      <th>quantity</th>\n",
       "      <th>location</th>\n",
       "      <th>water_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>('aabach', '2020-10-22')</td>\n",
       "      <td>G1</td>\n",
       "      <td>2020-10-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>aabach</td>\n",
       "      <td>Zurichsee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>('aabach', '2020-10-22')</td>\n",
       "      <td>G10</td>\n",
       "      <td>2020-10-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>aabach</td>\n",
       "      <td>Zurichsee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>('aabach', '2020-10-22')</td>\n",
       "      <td>G100</td>\n",
       "      <td>2020-10-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>aabach</td>\n",
       "      <td>Zurichsee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   loc_date  code        date  pcs_m  quantity location  \\\n",
       "0  ('aabach', '2020-10-22')    G1  2020-10-22    0.0         0   aabach   \n",
       "1  ('aabach', '2020-10-22')   G10  2020-10-22    0.0         0   aabach   \n",
       "2  ('aabach', '2020-10-22')  G100  2020-10-22    0.0         0   aabach   \n",
       "\n",
       "  water_name  \n",
       "0  Zurichsee  \n",
       "1  Zurichsee  \n",
       "2  Zurichsee  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggdf['p_code'] = aggdf['code'].map(lambda x: code_df.loc[x].p_code)\n",
    "aggs = {'date':'first',\n",
    "        'pcs_m':'sum',\n",
    "        'quantity':'sum',\n",
    "        'location':'first',\n",
    "        'water_name':'first',        \n",
    "}\n",
    "aggregated = aggdf.groupby(['loc_date', 'p_code'], as_index=False).aggregate(aggs)\n",
    "aggregated.rename(columns={'p_code':'code'}, inplace=True)\n",
    "aggregated.iloc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#008891\"> Save the aggregated values and post data directory to resources </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resources/surveydata/results_with_zeroes_aggregated_parent.csv\n"
     ]
    }
   ],
   "source": [
    "second_to_last_file_name = F\"{survey_data}/results_with_zeroes_aggregated_parent.csv\"\n",
    "print(second_to_last_file_name)\n",
    "data_directory.append(second_to_last_file_name )\n",
    "aggregated.to_csv(second_to_last_file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "putting: resources/data_directory.json\n"
     ]
    }
   ],
   "source": [
    "the_last_file_name = \"resources/data_directory.json\"\n",
    "ut.push_this_to_json(filename=the_last_file_name, data=data_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hopefully this just worked for you.\n",
    "\n",
    "If not contact analyst@hammerdirt.ch\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
