{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys, file and nav packages:\n",
    "# import os\n",
    "import datetime as dt\n",
    "# import csv\n",
    "# import json...\n",
    "\n",
    "# math packages:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# charting:\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# home brew utitilties\n",
    "import utilities.utility_functions as ut\n",
    "import utilities.abundance_classes as ac\n",
    "import utilities.chart_kwargs as ck\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "# set some parameters:\n",
    "today = dt.datetime.now().date().strftime(\"%Y-%m-%d\")\n",
    "start_date = '2020-04-01'\n",
    "end_date = today\n",
    "\n",
    "# the city, lake and river bassin we are aggregating to\n",
    "# the keys are column names in the survey data\n",
    "levels = {\"city\":\"Biel/Bienne\",\"water_name_slug\":'bielersee', \"river_bassin\":'aare'}\n",
    "\n",
    "# variables for the directory tree\n",
    "most_recent, survey_data, location_data, code_defs, stat_ent, geo_data, output = ut.make_local_paths()\n",
    "\n",
    "# name of the output folder:\n",
    "name_of_project = 'pop_inf_examp'\n",
    "\n",
    "# add the folder to the directory tree:\n",
    "project_directory = ut.make_project_folder(output, name_of_project)\n",
    "\n",
    "# keep track of output\n",
    "files_generated = []\n",
    "figure_num = 0\n",
    "data_num = 0\n",
    "\n",
    "def add_output(**kwargs):\n",
    "    files_generated.append({'tag':kwargs['tag'], 'number':kwargs['figure_num'], 'file':kwargs['file'],'type':kwargs['a_type']})\n",
    "    if kwargs['a_type'] == 'data':\n",
    "        kwargs['data'].to_csv(F\"{kwargs['file']}.csv\", index=False)\n",
    "    else:\n",
    "        plt.savefig(F\"{kwargs['file']}.jpeg\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#008891\">Population and infrastructure</span>\n",
    "\n",
    "#### <span style=\"color:#008891\">Does population and insfrasturcutre have a meausrable effect on the key indicators</span>\n",
    "\n",
    "The key indicators are common relationships used to provide insight to the most frequent questions using parameters that are taken directly from the survey data. The key indicators are relatively easy to calculate and interpret. In this section we explore the relationship between population and infrastructure and the key indicators for survey results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#008891\">Indicators for the most frequent questions</span>\n",
    "\n",
    "1. What do you find?\n",
    "2. How often do you find it?\n",
    "3. How much do you find?\n",
    "4. What else do you find?\n",
    "5. Where do you find the most?\n",
    "\n",
    "These are important questions to answer. The survey results are a census of objects that were primarily washed up on the beach. This helps answer the question:\n",
    "\n",
    "> **What are we likely to find at the beach?**\n",
    "\n",
    "\n",
    "> **What are we likely to find in the water?**\n",
    "\n",
    "The key indicators differ between regions and locations.  Which may mean that the extent and/or nature of the problem is different from one region to another.\n",
    "\n",
    "The reliability of these indicators is based on the following assumptions:\n",
    "\n",
    "1. The more trash there is on the ground the more a person is likely to find\n",
    "2. The survey results represent the minimum amount of trash at that site²\n",
    "3. For each survey: finding one item does not effect the chance of finding another³\n",
    "\n",
    "\n",
    "#### <span style=\"color:#008891\">More information </span>\n",
    "\n",
    "For information on a specific catchment area or a water feature see the notebook for that catchment area (or make one and send a pull request). For more information on the project visit https://www.plagespropres.ch/ .\n",
    "\n",
    "\n",
    "¹ The EU guide on monitoring marine litter https://mcc.jrc.ec.europa.eu/documents/201702074014.pdfhttps://mcc.jrc.ec.europa.eu/documents/201702074014.pdf<br> ² There is most likely more trash at the survey site, but certainly not less than what was recorded.<br>³ Independent observations : https://stats.stackexchange.com/questions/116355/what-does-independent-observations-meanhttps://stats.stackexchange.com/questions/116355/what-does-independent-observations-mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#008891\">A brief reminder of the survey method</span>\n",
    "\n",
    "A survey is a collection of observations. The observations correspond to the objects that were removed and counted during the survey. Each object is placed into one of 260 categories¹. The location, date, survey dimensions and the total number of objects in each category is noted.\n",
    "\n",
    "Some locations are sampled monthly, other were only sampled once. \n",
    "\n",
    "#### <span style=\"color:#008891\">The scope and size of the data for this example:</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name the data sources and where to find them\n",
    "# the easiest is to put them in the resources/most_recent directory\n",
    "# the extension default is resources/most_recent/\n",
    "data_sources = {    \n",
    "    \"survey_data\":\"results_with_zeroes.csv\",\n",
    "    \"river_bassins\":\"river_basins.json\",\n",
    "    \"beaches\":\"beaches_with_ranks.csv\",\n",
    "    \"codes\":\"mlw_codes.csv\",\n",
    "    \"code_groups\":\"code_group2.json\",    \n",
    "}\n",
    "\n",
    "# define the methods to use for the .JSON and .csv files:\n",
    "my_data_methods = {'json':ut.json_file_get, 'csv':pd.read_csv}\n",
    "\n",
    "# get your data:\n",
    "survey_data, river_bassins, dfBeaches, dfCodes, code_groups = ac.get_data_from_most_recent(data_sources, data_methods=my_data_methods)\n",
    "\n",
    "# format the date to timestamp and slice the data by start/end date\n",
    "dfSurveys = ac.fo_rmat_and_slice_date(survey_data.copy(), a_format=\"%Y-%m-%d\", start_date=start_date, end_date=end_date)\n",
    "\n",
    "# add the grouping column defined by river_bassins\n",
    "data = ac.add_a_grouping_column(dfSurveys, river_bassins, column_to_match=\"water_name_slug\")\n",
    "\n",
    "# set the index of the beach data to location slug\n",
    "dfBeaches.set_index('slug', inplace=True)\n",
    "\n",
    "# set the index of dfCodes to code:\n",
    "dfCodes.set_index('code', inplace=True)\n",
    "\n",
    "# make a map to the code descriptions\n",
    "code_description_map = dfCodes.description\n",
    "\n",
    "# put the data into a class\n",
    "# kwargs for the abundance class\n",
    "a_class_kwargs = dict(\n",
    "    code_group_data=code_groups,\n",
    "    levels=['river_bassin', 'water_name_slug', 'city'],\n",
    "    river_bassins=river_bassins,\n",
    "    exp_variables=['population','buildings', 'streets', 'intersects', 'pop_group_proj', 'pop_group_rip', 'streets_rank', 'buildings_rank'],       \n",
    "    code_group_loc=output,    \n",
    ")\n",
    "\n",
    "# the data labled by river bassin, water feature, city and beach name with independent variables attached\n",
    "a = ac.PreprocessData(data,  dfBeaches, these_cols=['loc_date', 'location', 'water_name_slug','date'], **a_class_kwargs)\n",
    "\n",
    "# define the final data set here:\n",
    "a_data = a.survey_data.copy()\n",
    "\n",
    "# identify all records with a quantity > 0\n",
    "a_data['fail'] = a_data.quantity > 0\n",
    "\n",
    "# thats it! all the survey records with the independent variables attached and columns to group by\n",
    "# date, location, (location, date), river bassin, water body, city, material, usage group, or object\n",
    "\n",
    "# convenience method\n",
    "def m_ap_code_to_description(data, key, func):\n",
    "    new_data = data.copy()\n",
    "    new_data['item'] = new_data.index.map(lambda x: func(x,key ))\n",
    "    new_data.set_index('item', inplace=True)\n",
    "    return new_data\n",
    "\n",
    "# describe the data set:\n",
    "num_obs = len(a_data)\n",
    "num_samps = len(a_data.loc_date.unique())\n",
    "num_obj = a_data.quantity.sum()\n",
    "num_locs = len(a_data.location.unique())\n",
    "\n",
    "i_s_na = a_data[a_data.isna().any(axis=1)]\n",
    "\n",
    "\n",
    "if len(i_s_na) > 0:\n",
    "    print(F\"There are {len(i_s_na)} records with NaN values.\\nCheck the column, levels and exp_variable definitions if you have made changes to default\\nCheck the availability of Geo data for locations in your selection.\")\n",
    "else:\n",
    "    print(\"\\nThere are no NaN values, good to go!\")\n",
    "    print(F\"\\n Results for all surveys between {start_date} and {end_date} from the following catchment areas:\\n\\n  {a_data.river_bassin.unique()}\")\n",
    "    print(F\"\\nThere are {'{:,}'.format(num_obs)} observations from {num_samps} surveys.\\n\\nThere were {'{:,}'.format(num_obj)} objects collected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the daily survye totals by pcs_m and make a map to loc_date:\n",
    "a_daily_totals = ac.fo_rmat_and_slice_date(a.daily_totals_all, a_format=\"%Y-%m-%d\", start_date=start_date, end_date=end_date)\n",
    "a_daily_totals.set_index(\"loc_date\", inplace=True)\n",
    "m_ap_to_survey_totals = a_daily_totals.pcs_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_data.groupname.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_data['month'] = a_data.loc_date.map(lambda x: int(x[1][5:7]))\n",
    "\n",
    "use_these_cols = [\n",
    "    'loc_date','date', 'month', 'location', 'water_name_slug','river_bassin',  'city','population',\n",
    "    'buildings', 'streets', 'intersects', 'pop_group_proj', 'pop_group_rip',\n",
    "]\n",
    "\n",
    "# the values for these object codes will be combined for each survey\n",
    "method = 'dense'\n",
    "groupname = ['tobacco']\n",
    "a_col = 'pcs_m'\n",
    "a_code = a_data.loc[(a_data.groupname.isin(groupname))&(a_data.quantity > 0)].copy()\n",
    "a_code_df = a_code.groupby(use_these_cols, as_index=False).agg({'pcs_m':'sum', 'quantity':'sum'})\n",
    "\n",
    "# get the fail rate\n",
    "a_code_df['fail'] = a_code_df.quantity > 0\n",
    "\n",
    "# map the survey total to each record \n",
    "a_code_df['survey_total'] = a_code_df.loc_date.map(lambda x: m_ap_to_survey_totals.loc[[x]][0])\n",
    "a_code_df['% of total'] = a_code_df.quantity/a_code_df.survey_total\n",
    "\n",
    "ar_basin = a_code_df[a_code_df.river_bassin == 'aare'].copy()\n",
    "\n",
    "ar_basin.drop(['pop_group_proj','pop_group_rip'], axis=1, inplace=True)\n",
    "\n",
    "ar_basin['pop_rank'] =ar_basin.population.rank(method=method)\n",
    "ar_basin['street_rank']=ar_basin.streets.rank(method=method)\n",
    "ar_basin['bild_rank'] = ar_basin.buildings.rank(method=method)\n",
    "ar_basin['int_rank'] = ar_basin.intersects.rank(method=method)\n",
    "\n",
    "ar_basin['sum_ranks'] = ar_basin.bild_rank + ar_basin.street_rank + ar_basin.pop_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = ar_basin, x='pop_rank', y=a_col, hue='bild_rank', palette='rocket_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = ar_basin, x='bild_rank', y=a_col, hue='pop_rank', palette='rocket_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = ar_basin, x='street_rank', y=a_col,  hue='pop_rank', palette='rocket_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = ar_basin, x='int_rank', y=a_col, hue='pop_rank', palette='rocket_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = ar_basin, x='sum_ranks', y=a_col, hue='pop_rank', palette='rocket_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = ar_basin, x='month', y=a_col, hue='pop_rank', palette='rocket_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author = \"roger@hammerdirt.ch\"\n",
    "my_message = \"Statistics is fun when you do it outside\"\n",
    "print(F\"\\nProduced by: {author}\\nDate: {today}\\n\\n{my_message}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
