{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">code group summary --- still testing mostly fucntional</span>\n",
    "\n",
    "### <span style=\"color:red\">Beach litter survey results 2020/2021 Switzerland</span>\n",
    "\n",
    "#### <span style=\"color:#008891\">Definition of significant values</span>\n",
    "\n",
    "#### <span style=\"color:#008891\">National survey result </span>\n",
    "\n",
    "#### <span style=\"color:#008891\">Lac Léman results</span>\n",
    "\n",
    "#### <span style=\"color:#008891\">Lac Léman code group utilisation and availability </span>\n",
    "\n",
    "\n",
    "questions or comments: analyst@hammerdirt.ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys, file and nav packages:\n",
    "import os\n",
    "import datetime as dt\n",
    "import csv\n",
    "\n",
    "# math packages:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import datetime as dt \n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from scipy.stats import kendalltau, pearsonr, spearmanr\n",
    "\n",
    "def kendall_pval(x,y):\n",
    "    return kendalltau(x,y)[1]\n",
    "\n",
    "def pearsonr_pval(x,y):\n",
    "    return pearsonr(x,y)[1]\n",
    "\n",
    "def spearmanr_pval(x,y):\n",
    "    return spearmanr(x,y)[1]\n",
    "\n",
    "\n",
    "\n",
    "# charting:\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib import ticker\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gridspec\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# mapping\n",
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "# home brew utitilties\n",
    "import utilities.utility_functions as ut\n",
    "\n",
    "# documenting\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "\n",
    "# variables/arrays that are frequently used:\n",
    "# project lakes\n",
    "the_lakes = [\n",
    "    \"Bielersee\",\n",
    "    \"Walensee\",\n",
    "    \"Lac Léman\",\n",
    "    \"Zurichsee\",\n",
    "    \"Neuenburgersee\",\n",
    "    \"Thunersee\",\n",
    "    \"Lago Maggiore\",\n",
    "    \"Brienzersee\",\n",
    "]\n",
    "\n",
    "# standard formats already in use for charts, these will gradually\n",
    "# define the chart style or output format for the app\n",
    "# you can just apply these as kwargs to different elements...\n",
    "\n",
    "\n",
    "# table kwargs\n",
    "table_k = dict(loc=\"top left\", bbox=(0,0,1,1), colWidths=[.5, .5], cellLoc='center')\n",
    "tablecenter_k = dict(loc=\"top left\", bbox=(0,0,1,1), cellLoc='center')\n",
    "tabtickp_k = dict(axis='both', which='both', bottom=False, top=False, left=False, right=False, labelleft=False, labelbottom=False)\n",
    "\n",
    "# chart kwargs\n",
    "title_k = {'loc':'left', 'pad':10, 'linespacing':1.5, 'fontsize':12}\n",
    "title_k20 = {'loc':'left', 'pad':10, 'linespacing':1.5, 'fontsize':12, 'color':'dodgerblue'}\n",
    "title_k17 = {'loc':'left', 'pad':10, 'linespacing':1.5, 'fontsize':12, 'color':'salmon'}\n",
    "titler_k20 = {'loc':'right', 'pad':10, 'linespacing':1.5, 'fontsize':12, 'color':'dodgerblue'}\n",
    "titler_k17 = {'loc':'right', 'pad':10, 'linespacing':1.5, 'fontsize':12, 'color':'salmon'}\n",
    "xlab_k = {'labelpad':10, 'fontsize':12}\n",
    "ylab_k = {'labelpad':14, 'fontsize':14}\n",
    "titler_k = {'loc':'right', 'pad':10, 'linespacing':1.5, 'fontsize':12}\n",
    "\n",
    "# use these to format date axis in charts\n",
    "weeks = mdates.WeekdayLocator(byweekday=1, interval=4)\n",
    "days = mdates.DayLocator(bymonthday=1, interval=1)\n",
    "months = mdates.MonthLocator(bymonth=[3,6,9,12])\n",
    "wks_fmt = mdates.DateFormatter('%d')\n",
    "mths_fmt = mdates.DateFormatter('%b')\n",
    "\n",
    "# map marker size:\n",
    "markerSize = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the local file structure. The resources are located in the corresponding directory.\n",
    "survey_data, location_data, code_defs, stat_ent, geo_data, output = ut.make_local_paths()\n",
    "\n",
    "# set some parameters:\n",
    "start_date = '2020-04-01'\n",
    "end_date = dt.datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# define a code group, there are predefined code groups in\n",
    "# the resources folder under mlwcodedefs\n",
    "const = ut.json_file_get(F\"{output}/code_groups/construction.json\")\n",
    "micro = ut.json_file_get(F\"{output}/code_groups/codeListMicros.json\")\n",
    "steps = ut.json_file_get(F\"{output}/code_groups/wastewater.json\")\n",
    "group_one = steps\n",
    "group_one_name = \"waste water\"\n",
    "code = \"step\"\n",
    "group_names = [group_one_name]\n",
    "\n",
    "\n",
    "\n",
    "# decide which data to use\n",
    "aggregated = False \n",
    "\n",
    "\n",
    "# define a second group or leave as all\n",
    "# this note book will only compare the two groups \n",
    "group_two = micro\n",
    "group_two_name = 'less than 5mm'\n",
    "code = \"micro\"\n",
    "\n",
    "# define a third group or leave as all\n",
    "# this note book will only compare the two groups \n",
    "group_three = const\n",
    "group_three_name = 'construction'\n",
    "code = \"const\"\n",
    "\n",
    "# keep track of the figures you produce\n",
    "table_num = 1\n",
    "figure_num = 1\n",
    "map_num = 1\n",
    "\n",
    "\n",
    "# choose a lake:\n",
    "lake = 'Lac Léman'\n",
    "\n",
    "# define a significant event:\n",
    "sig = .9\n",
    "one_minus_sig = (1-sig)\n",
    "\n",
    "# define explanatory variables:\n",
    "expv = ['population','streets','buildings','rivs']\n",
    "\n",
    "# name the folder:\n",
    "name_of_project = 'testproject'\n",
    "\n",
    "# use this to store things:\n",
    "project_directory = ut.make_project_folder(output, name_of_project)\n",
    "\n",
    "# probably want to keep these... the works already done\n",
    "# aggregated survey data\n",
    "dfAgg = pd.read_csv(F\"{survey_data}/results_with_zeroes_aggregated_parent.csv\")\n",
    "dfAgg['date'] = pd.to_datetime(dfAgg['date'])\n",
    "\n",
    "# non aggregated survey data\n",
    "dfSurveys = pd.read_csv(F\"{survey_data}/results_with_zeroes.csv\")\n",
    "dfSurveys['date'] = pd.to_datetime(dfSurveys['date'])\n",
    "\n",
    "# beach data\n",
    "dfBeaches = pd.read_csv(F\"{location_data}/beaches_with_ranks.csv\")\n",
    "dfBeaches.set_index('slug', inplace=True)\n",
    "dfBeaches.rename(columns={\"NUMPOINTS\":\"intersects\"}, inplace=True)\n",
    "\n",
    "# code definitions\n",
    "dfCodes = pd.read_csv(F\"{code_defs}/mlw_codes.csv\")\n",
    "cols_to_keep = ['loc_date',\n",
    "                'location',\n",
    "                'water_name',\n",
    "                'date',\n",
    "                'population',\n",
    "                'streets',\n",
    "                'buildings',\n",
    "                'rivs',\n",
    "                'pop_builds',\n",
    "                'pop_streets',\n",
    "                'streets_rivs',\n",
    "                \n",
    "               ]\n",
    "\n",
    "# geo data: explantory variables, index by slug and make a map:\n",
    "dfStreets = pd.read_csv(F\"{geo_data}/exp_variables/strasse_1000.csv\", index_col='slug')['length']\n",
    "dfBlds = pd.read_csv(F\"{geo_data}/exp_variables/builds_500.csv\", index_col='slug')['area']\n",
    "dfRivs = dfBeaches['intersects']\n",
    "\n",
    "# restrict to lakes only\n",
    "# aggregated to the parent code, which is an MLW code\n",
    "dfS = dfAgg.loc[(dfAgg.water_name.isin(the_lakes))&(dfAgg.date >= start_date)].copy()\n",
    "dfNag = dfSurveys.loc[(dfSurveys.water_name.isin(the_lakes))&(dfSurveys.date >= start_date)].copy()\n",
    "thesecols = ['loc_date',\n",
    "             'location',\n",
    "             'water_name',\n",
    "             'date']\n",
    "dfNagl = dfNag.copy()\n",
    "mapG82 = dfNagl[dfNagl.code.isin(['G82', 'G912'])].groupby(thesecols, as_index=False).agg({'pcs_m':'sum', 'quantity':'sum'})\n",
    "mapG82['code'] = 'G82'\n",
    "mapG81 = dfNagl[dfNagl.code.isin(['G81', 'G911'])].groupby(thesecols, as_index=False).agg({'pcs_m':'sum', 'quantity':'sum'})\n",
    "mapG81['code'] = 'G81'\n",
    "mapG74 = dfNagl[dfNagl.code.isin(['G74', 'G910', 'G909'])].groupby(thesecols, as_index=False).agg({'pcs_m':'sum', 'quantity':'sum'})\n",
    "mapG74['code'] = 'G74'\n",
    "dfnofoam = dfNag.loc[~dfNag.code.isin(['G82', 'G912','G81', 'G911','G74', 'G910', 'G909'])]\n",
    "newdf = pd.concat([dfnofoam,mapG74,mapG81,mapG82])\n",
    "\n",
    "# map geo values to aggregated survey results:\n",
    "dfS['population']=dfS.location.map(lambda x: dfBeaches.loc[x]['population'])\n",
    "dfS['streets'] = dfS.location.map(lambda x: dfStreets.loc[x])\n",
    "dfS['buildings'] = dfS.location.map(lambda x: dfBlds.loc[x])\n",
    "dfS['rivs'] = dfS.location.map(lambda x: dfRivs.loc[x].values[0])\n",
    "\n",
    "dfS['pop_streets'] = dfS.population + dfS.streets\n",
    "dfS['pop_builds'] = dfS.population + dfS.buildings\n",
    "dfS['streets_rivs'] = dfS.streets + dfS.rivs\n",
    "\n",
    "# map geo values to non aggregated survey results:\n",
    "newdf['population']=newdf.location.map(lambda x: dfBeaches.loc[x]['population'])\n",
    "newdf['streets'] = newdf.location.map(lambda x: dfStreets.loc[x])\n",
    "newdf['buildings'] = newdf.location.map(lambda x: dfBlds.loc[x])\n",
    "newdf['rivs'] = newdf.location.map(lambda x: dfRivs.loc[x].values[0])\n",
    "\n",
    "newdf['pop_streets'] = newdf.population + newdf.streets\n",
    "newdf['pop_builds'] = newdf.population + newdf.buildings\n",
    "newdf['streets_rivs'] = newdf.streets + newdf.rivs\n",
    "# get a list of the codes in the current data\n",
    "codes_in_use = dfS.code.unique()\n",
    "\n",
    "if aggregated:\n",
    "    print(\"Using aggregated data\")\n",
    "    useThis = dfS.copy()\n",
    "else:\n",
    "    print(\"Using non aggregated data\")\n",
    "    useThis = newdf.copy()\n",
    "    \n",
    "\n",
    "# seperate out group_one:\n",
    "dfGone = useThis.loc[useThis.code.isin(group_one)].copy()\n",
    "\n",
    "# group all the codes in group 2 or all the codes not in group one\n",
    "if 'all' in group_two:\n",
    "    dfGtwo = useThis.loc[~useThis.code.isin(group_one)].copy()\n",
    "    group_two_name = 'the rest'\n",
    "else: \n",
    "    dfGtwo = useThis.loc[useThis.code.isin(group_two)].copy()\n",
    "group_names.append(group_two_name)\n",
    "# get the codes that have been accounted for in groupone and grouptwo:\n",
    "codes_accounted_for = set(list(dfGone.code.unique())) | set(list(dfGtwo.code.unique()))\n",
    "if 'all' in group_three:\n",
    "    group_three_name = 'the rest'    \n",
    "    # get the codes that have not been accounted for\n",
    "    group_three = [x for x in codes_in_use if x not in codes_accounted_for]\n",
    "    dfGthree = useThis.loc[useThis.code.isin(group_three)].copy()\n",
    "else:\n",
    "    dfGthree = useThis.loc[useThis.code.isin(group_three)].copy()\n",
    "#     groupdfs = [dfGone,dfGtwo, dfGthree]\n",
    "group_names.append(group_three_name)\n",
    "    \n",
    "\n",
    "\n",
    "# gather up the created dataframes\n",
    "groupdfs = [dfGone,dfGtwo, dfGthree]\n",
    "\n",
    "\n",
    "\n",
    "# get the codes that have not been accounted for\n",
    "# group_three = [x for x in codes_in_use if x not in codes_accounted_for]\n",
    "\n",
    "# # make a boolean to alert to the presence of data in group three\n",
    "# # if you choose all as the second group then groupthree will be empty\n",
    "# gthree = len(group_three) > 0\n",
    "\n",
    "# if gthree:\n",
    "#     group_three_name = 'the rest'\n",
    "#     dfGthree = useThis.loc[useThis.code.isin(group_three)].copy()\n",
    "#     project_groups = [group_one, group_two, group_three]\n",
    "#     group_names.append(group_three_name)\n",
    "#     groupdfs.append(dfGthree)\n",
    "# else:\n",
    "#     project_groups = [group_one, group_two]\n",
    "\n",
    "\n",
    "# keep track of the files you are exporting:\n",
    "files_generated = []\n",
    "\n",
    "# save files\n",
    "survey_csv = F\"{project_directory}/survey_data.csv\"\n",
    "files_generated.append(survey_csv)\n",
    "useThis.to_csv(survey_csv, index=False)\n",
    "\n",
    "beaches_csv = F\"{project_directory}/beach_data.csv\"\n",
    "files_generated.append(beaches_csv)\n",
    "dfBeaches.to_csv(beaches_csv, index=False)\n",
    "\n",
    "\n",
    "# daily totals for each group\n",
    "# and assign the name 'rest' to either group_two or group_three\n",
    "# that depends on the value of group_two\n",
    "\n",
    "# put the dfs in a dictionary and key to group name\n",
    "groupdfs = {x:groupdfs[i] for i,x in enumerate(group_names)}\n",
    "\n",
    "# daily totals for each group\n",
    "# name the columns to keep when aggregating:\n",
    "\n",
    "\n",
    "# get the daily total for each group and store in a data frame:\n",
    "for name in group_names:\n",
    "    new_name = F\"{name}_dt\"\n",
    "    dtdf = groupdfs[name].groupby(cols_to_keep, as_index=False).agg({\"pcs_m\":\"sum\", \"quantity\":\"sum\"})    \n",
    "    dtdf.set_index('loc_date', inplace=True)\n",
    "    groupdfs.update({new_name:dtdf})\n",
    "\n",
    "# get each survey total per location and date (all codes included)\n",
    "# we can check combined values with this\n",
    "allDf = useThis.groupby(['loc_date','location','water_name', 'date','population','streets','buildings','rivs'], as_index=False).agg({\"pcs_m\":\"sum\", \"quantity\":\"sum\"})\n",
    "allDf.set_index('loc_date', inplace=True)\n",
    "\n",
    "# add that to the collection of dataframes\n",
    "groupdfs.update({\"dfDtAll\":allDf})\n",
    "groupkeys = groupdfs.keys()\n",
    "\n",
    "# get the percent of survey total for each group\n",
    "dt_names = [F\"{name}_dt\" for name in group_names]\n",
    "dt_names.append(\"dfDtAll\")\n",
    "   \n",
    "for name in dt_names:\n",
    "    groupdfs[name][\"p_total\"] = groupdfs[name].index.map(lambda x: groupdfs[name].loc[x].pcs_m/groupdfs[\"dfDtAll\"].loc[x].pcs_m)\n",
    "    groupdfs[name]['p_total'] = groupdfs[name]['p_total']*100\n",
    "    groupdfs[name]['p_total'] = groupdfs[name]['p_total'].round(2)\n",
    "\n",
    "# summarize the results by lake.\n",
    "# Number of locations and surveys with or without code for each lake\n",
    "# Make column headers plain english, make table\n",
    "\n",
    "# place to store the summaries\n",
    "regional_summaries = []\n",
    "\n",
    "# creating a summary for each df in groupdfs\n",
    "for name in dt_names:\n",
    "    this_data = groupdfs[name]\n",
    "    this_agg = {'loc_date':'count', 'location':'nunique', 'pcs_m':'median', 'p_total':'mean'}\n",
    "    this_data.reset_index(inplace=True)\n",
    "    regional_summary = this_data.groupby('water_name').agg(this_agg)\n",
    "    regions = list(regional_summary.index)\n",
    "    \n",
    "    # the mean of all the group data \n",
    "    rs_mean = this_data.groupby('water_name').pcs_m.mean()\n",
    "    \n",
    "    # the number of samples with group pcs_m > 0\n",
    "    regional_summary[\"# samps with\"] = regional_summary.index.map(lambda x: F\"{this_data[(this_data.water_name == x)&(this_data.pcs_m > 0)].pcs_m.count()}/{this_data[this_data.water_name == x].loc_date.nunique()}\")\n",
    "    \n",
    "    hasGpi = this_data[this_data.quantity > 0]\n",
    "    # count the number of locations with the code value per lake\n",
    "    lakes_with = hasGpi[['water_name', 'location']].groupby(['water_name']).nunique('location')\n",
    "    \n",
    "    regional_summary['locations with'] = regional_summary.index.map(lambda x: F\"{lakes_with.loc[x].location}/{regional_summary.loc[x].location}\")\n",
    "    \n",
    "    # rename the columns to ui style\n",
    "    new_columns = {'loc_date':'# samples',\n",
    "                   \"locations with\":\"# with\",\n",
    "                   'location':'# locations',\n",
    "                   \"pcs_m\":\"median pcs/m\",\n",
    "                   \"p_total\":\"% of daily total\"\n",
    "                  }\n",
    "    regional_summary.rename(columns=new_columns, inplace=True)\n",
    "    \n",
    "    # add the pcs/m per region:\n",
    "    regional_summary['mean pcs/m'] = regional_summary.index.map(lambda x: rs_mean[x])\n",
    "    col_order = [\"# locations\", \"# with\",\"# samples\", \"# samps with\", \"median pcs/m\", \"mean pcs/m\", \"% of daily total\"]\n",
    "    regional_summary = regional_summary[col_order]\n",
    "    \n",
    "    # reset the index, we need the index for a column in the table\n",
    "    regional_summary.reset_index(inplace=True)\n",
    "\n",
    "    # make a column name for the former index:\n",
    "    regional_summary.rename(columns={'water_name':'lake'}, inplace=True)\n",
    "\n",
    "    # round any values:\n",
    "    regional_summary['mean pcs/m'] = regional_summary['mean pcs/m'].round(3)\n",
    "    regional_summary[\"median pcs/m\"] = regional_summary[\"median pcs/m\"].round(3)\n",
    "    regional_summary[\"% of daily total\"] = regional_summary[\"% of daily total\"].round(2)\n",
    "    regional_summaries.append({'data':regional_summary.values, 'cols':regional_summary.columns})\n",
    "\n",
    "# significant values\n",
    "# use the ECDF method from statsmodels to get the ecdf function for the different data frames\n",
    "# pieces per meter\n",
    "pmecdfs = {}\n",
    "for name in dt_names:\n",
    "    somdata = groupdfs[name]\n",
    "    thisecdf = ECDF(somdata['pcs_m'])\n",
    "    pmecdfs.update({name:thisecdf})\n",
    "    somdata['p_dt'] = somdata.pcs_m.map(lambda x: 1-thisecdf(x))\n",
    "    somdata['significant'] = somdata.p_dt.map(lambda x: x <= one_minus_sig)\n",
    "    \n",
    "ptecdfs = {}\n",
    "for name in dt_names:\n",
    "    somdata = groupdfs[name]\n",
    "    thisecdf = ECDF(somdata['p_total'])    \n",
    "    ptecdfs.update({name:thisecdf})\n",
    "    somdata['p_pt'] = somdata.p_total.map(lambda x: 1-thisecdf(x))\n",
    "    somdata['significant_p'] = somdata.p_pt.map(lambda x: x <= one_minus_sig)\n",
    "\n",
    "events_data = {}\n",
    "for name in dt_names:\n",
    "    somdata = groupdfs[name]\n",
    "    p_dt90 = somdata[somdata.significant == True].groupby(['water_name'], as_index=False).loc_date.nunique()\n",
    "    p_dt90['frequency'] = p_dt90.water_name.map(lambda x: (F\"{p_dt90.loc[p_dt90.water_name == x]['loc_date'].values[0]}/{regional_summary.loc[regional_summary.lake == x]['# samples'].values[0]}\"))\n",
    "    p_dt90.rename(columns={'loc_date':'# significant','water_name': 'lake'}, inplace=True)\n",
    "    events_data.update({name:p_dt90})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#1e90ff\">Methods</span>\n",
    "\n",
    "#### **<span style=\"color:#008891\">Data source, time frame, geographic scope and collection methods </span>**\n",
    "\n",
    "The data for this analysis is the results from beach-litter surveys conducted in Switzerland from <span style=\"color:red\">!! Put your dates here !! through January 18, 2021</span>.\n",
    "\n",
    "All surveys that were conducted on Bielersee, Neuenburgersee, Thunersee, Walensee, Zurichsee, Lac Léman, Brienzersee and Lago Magiore were considered. \n",
    "\n",
    "The data was collected according to the protocol described here [https://www.plagespropres.ch/](https://www.plagespropres.ch/). In brief all visible data is collected along a beach within a measured distance from the waters edge. The width of the survey area depends on the terrain and the water level. The visible strand line or the nearest physical structure defines the width of a survey.\n",
    "\n",
    "Surveys were conducted by the following organizations:\n",
    "\n",
    "1. hammerdirt\n",
    "2. Association pour le Sauvegarde du leman\n",
    "3. Solid Waste Management Ecole Polytechnique Federal\n",
    "4. Ecole International de Geneve\n",
    "5. Precious plastic leman\n",
    "6. Why isn't your association here?\n",
    "\n",
    "This analysis is an open source document. The working note book is available in the repository located here [https://github.com/hammerdirt-analyst/iqals](https://github.com/hammerdirt-analyst/iqals).\n",
    "\n",
    "**Francais**\n",
    "\n",
    "Les données utilisées pour cette analyse sont les résultats d'enquêtes sur les déchets de plage menées en Suisse <span style=\"color:red\">!! put your dates here !! du 1er avril 2020 au 28  janvier 2021</span>.\n",
    "\n",
    "Toutes les enquêtes qui ont été menées sur le lac de Bienne, le lac de Neuchâtel, le lac de Thoune, le lac Walensee, le lac de Zurich, le lac Léman, le lac Brienzer et le lac Magiore ont été prises en compte. \n",
    "\n",
    "Les données ont été collectées selon le protocole décrit ici [https://www.plagespropres.ch/](https://www.plagespropres.ch/). En bref, toutes les données visibles sont collectées le long d'une plage à une distance mesurée du bord de l'eau. La largeur de la zone d'étude dépend du terrain et du niveau de l'eau. La ligne de rive visible ou la structure physique la plus proche définit la largeur d'une enquête.\n",
    "\n",
    "Des enquêtes ont été menées par:\n",
    "\n",
    "1. hammerdirt\n",
    "2. Association pour le Sauvegarde du leman\n",
    "3. Solid Waste Management Ecole Polytechnique Federal\n",
    "4. Ecole International de Geneve\n",
    "5. Precious plastic leman\n",
    "6. Why isn't your association here?1. hammerdirt\n",
    "\n",
    "Cette analyse est un document open source. Le cahier de notes de travail est disponible dans le dépôt situé ici: [https://github.com/hammerdirt-analyst/iqals](https://github.com/hammerdirt-analyst/iqals)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#008891\">Scope of surveys</span>\n",
    "\n",
    "The scope of the surveys includes the total population levels and land use configuration for the adjacent municipality. Land use is quantified by calculating the m² of space attributed to buildings wtihin 500m of the survey location and the total length in meters of roads within 1000m of the survey location\\*.\n",
    "\n",
    "The results and survey locations can be classified according to the following attributes:\n",
    "\n",
    "1. M² of buildings within 500m of the survey\n",
    "2. Total length of streets/roads within 1000m of the survey\n",
    "3. Number of river/canal intersections within 1500m of the survey\n",
    "3. Population of the surounding municipality\n",
    "\n",
    "\\*Values are calculated using [https://shop.swisstopo.admin.ch/en/products/landscape/tlm3D](https://shop.swisstopo.admin.ch/en/products/landscape/tlm3D)\n",
    "\n",
    "**Français**\n",
    "\n",
    "Le champ d'application des enquêtes comprend les niveaux de population totale et la configuration de l'utilisation des sols pour la municipalité environante. L'utilisation des sols est quantifiée en calculant la superficie au sol en m² attribuée aux bâtiments dans un rayon de 500 m du lieu de l'enquête et la longueur totale en mètres des routes dans un rayon de 1000 m du lieu de l'enquête\\*.\n",
    "\n",
    "Les résultats et les lieux d'enquête peuvent être classés selon les attributs suivants :\n",
    "\n",
    "1. M² de surface au sol des bâtiments situés dans un rayon de 500m du lieu d'enquête\n",
    "2. Longueur totale des rues/routes dans un rayon de 1000 m de l'enquête\n",
    "3. Nombre d'intersections de rivières/canaux dans un rayon de 1500m de l'étude\n",
    "3. Population de la commune environnante\n",
    "\n",
    "Les valeurs sont calculées en utilisant  [https://shop.swisstopo.admin.ch/en/products/landscape/tlm3D](https://shop.swisstopo.admin.ch/en/products/landscape/tlm3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#008891\">Correlation of survey results to attribute values</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupdfs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#008891\">Location attribute values and survey totals</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the totals per survey location are here :groupdfs['dfDtAll']\n",
    "# report the number of surveys per popualtion ranking\n",
    "somquints = {}\n",
    "somdata = groupdfs['dfDtAll']\n",
    "for var in expv:\n",
    "    \n",
    "#     pop_surv_count = \n",
    "#     \n",
    "    popquints = np.percentile(somdata[var], [20,40,60, 80])\n",
    "    ranges = [(0,popquints[0])]\n",
    "    ranges = [*ranges,*[(popquints[i], popquints[i+1]) for i,x in enumerate(popquints[:-1])]]\n",
    "    ranges.append((popquints[-1], 1000000))\n",
    "    somquints.update({var:ranges})\n",
    "\n",
    "# print(total_surveys)\n",
    "\n",
    "exptables = {}\n",
    "for i, a_var in enumerate(expv):\n",
    "    print(a_var)\n",
    "    surveys_population = []\n",
    "    ranges = somquints[a_var]\n",
    "    newdata = somdata.groupby(a_var).loc_date.count()\n",
    "    total_surveys = newdata.sum()\n",
    "    end = len(ranges)-1\n",
    "    for i,a_range in enumerate(ranges):\n",
    "        if i == end:\n",
    "            a_label = F\"{'{:,}'.format(np.int(a_range[0]))} - \"\n",
    "        else:\n",
    "            a_label = F\"{'{:,}'.format(np.int(a_range[0]))} - {'{:,}'.format(np.int(a_range[1]))}\"\n",
    "        a_count = newdata[(newdata.index >= a_range[0])&(newdata.index < a_range[1])].sum()\n",
    "        a_median = somdata[(somdata[a_var] >= a_range[0])&(somdata[a_var] < a_range[1])].pcs_m.median()\n",
    "        a_mean = somdata[(somdata[a_var] >= a_range[0])&(somdata[a_var] < a_range[1])].pcs_m.mean()\n",
    "        a_pt = np.round((a_count/total_surveys)*100,2)\n",
    "        a_row = {F\"{a_var} range\":a_label, '# samples': a_count,'% of total': a_pt, 'median pcs/m':np.round(a_median,2), 'avg pcs/m':np.round(a_mean, 2)}\n",
    "        surveys_population.append(a_row)\n",
    "    describe_pop = pd.DataFrame(surveys_population)\n",
    "    exptables.update({a_var:describe_pop})\n",
    "\n",
    "# adjust table args if need:\n",
    "tablecenter_k = dict(loc=\"top left\", bbox=(0,0,1,1), cellLoc='center', fontsize=12)\n",
    "\n",
    "col_labels = {\n",
    "    'population':'population',\n",
    "    'streets': 'meters of streets',\n",
    "    'buildings': 'meters² of buildings',\n",
    "    'rivs': 'canal\\\\river intersections'\n",
    "    }\n",
    "\n",
    "\n",
    "for i, name in enumerate(expv):\n",
    "    fig, axs = plt.subplots(figsize=(10,6))\n",
    "    axs.add_table(mpl.table.table(cellText=exptables[name].values , colLabels=exptables[name].columns,colColours=['antiquewhite' for col in exptables[name].columns], ax=axs, **tablecenter_k))\n",
    "    axs.grid(False)\n",
    "    axs.spines[\"top\"].set_visible(False)\n",
    "    axs.spines[\"right\"].set_visible(False)\n",
    "    axs.spines[\"bottom\"].set_visible(False)\n",
    "    axs.spines[\"left\"].set_visible(False)\n",
    "    axs.tick_params(**tabtickp_k)\n",
    "    axs.set_title(F\"Table {table_num}: Survey results by {col_labels[name]} ranking\", **titler_k)\n",
    "\n",
    "\n",
    "# tabletwofile = F\"{project_directory}/tabletwo.jpg\"\n",
    "# files_generated.append(tabletwofile)\n",
    "# plt.savefig(tabletwofile, dpi=300)\n",
    "    table_num += 1\n",
    "    plt.tight_layout()\n",
    "    plt.box(on=None)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "somdata = groupdfs['dfDtAll']\n",
    "g = sns.PairGrid(somdata[['population',\n",
    "                'streets',\n",
    "                'buildings','rivs','pcs_m', 'water_name']], diag_sharey=False, corner=True, hue='water_name', palette='husl')\n",
    "g.map_lower(sns.scatterplot)\n",
    "g.map_diag(sns.boxplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanscoef = somdata[['population', 'buildings', 'streets', 'rivs', 'pcs_m']].corr('spearman')\n",
    "maskspearmans= np.zeros_like(spearmanscoef)\n",
    "maskspearmans[np.triu_indices_from(maskspearmans, 1)] = True\n",
    "\n",
    "spearmanspval = somdata[['population', 'buildings', 'streets', 'rivs', 'pcs_m']].corr(method=spearmanr_pval)\n",
    "maskpval= np.zeros_like(spearmanspval)\n",
    "maskpval[np.triu_indices_from(maskpval, 1)] = True\n",
    "fig, axs = plt.subplots(1,2, figsize=(12,6))\n",
    "\n",
    "axs[0] = sns.heatmap(spearmanscoef, mask=maskspearmans, vmax=1, vmin=-1, square=True, annot=True, ax=axs[0], cmap='flare_r')\n",
    "axs[0].set_title(\"Spearman ranked correlation coefficient\", **title_k)\n",
    "axs[1] = sns.heatmap(spearmanspval, mask=maskpval, vmax=1, vmin=0, square=True, annot=True, ax=axs[1], cmap='flare_r')\n",
    "axs[1].set_title(\"P value spearman ranked correlation coefficient\", **title_k)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure one total survey pcs/m per attribute value:\n",
    "\n",
    "\n",
    "# key the column names to appropriate chart titles:\n",
    "titles_text = {\n",
    "    'population':'Municipal population',\n",
    "    'streets': 'Meters of roads/streets',\n",
    "    'buildings':'Meters squared of buildings',\n",
    "    'rivs':'River/canal intersection',    \n",
    "}\n",
    "\n",
    "# produce a chart for each explanatory variable and each group\n",
    "# for i,name in enumerate(dt_names):\n",
    "#     fig, axs = plt.subplots(1,len(expv), figsize=(14,4), sharey=True)\n",
    "#     for j,att in enumerate(expv):\n",
    "#         data = groupdfs[name][['water_name','pcs_m', att]]\n",
    "#         ax = sns.scatterplot(data=data, x=att, y='pcs_m', hue='water_name', palette='husl', alpha=0.8, ax=axs[j])\n",
    "#         axs[j].set_title(F\"{att}\", **title_k)\n",
    "#         axs[j].set_xlabel(titles_text[att], **xlab_k)\n",
    "#         axs[j].set_ylabel('')\n",
    "#         axs[j].get_legend().remove()\n",
    "#     axs[0].set_ylabel('Survey total pieces per meter', **xlab_k)\n",
    "\n",
    "#     plt.suptitle(F\"Figure {figure_num}: survey total {name[:-3]}, by attribute and lake: {end_date}\", x=0.02, y=0.99, ha='left')\n",
    "#     figure_num += 1\n",
    "#     # save that\n",
    "#     figureonefile = F\"{project_directory}/figure{figure_num}.jpg\"\n",
    "#     files_generated.append(figureonefile)\n",
    "#     plt.savefig(figureonefile, dpi=300)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# calculator: what ever other metric that needs to be calculated\n",
    "# shoud be calculated here. Then they can be used in the narrative\n",
    "# or applied to the regional results. Good place to look at\n",
    "# correlation tables with geo data\n",
    "\n",
    "# group by location and get the total found\n",
    "# qGpi = dfCode.groupby(['location', 'water_name'], as_index=False).quantity.sum()\n",
    "\n",
    "# seperate locations with or wothout the code\n",
    "# noGpi = qGpi[qGpi.quantity == 0]\n",
    "# hasGpi = qGpi[qGpi.quantity > 0]\n",
    "\n",
    "# count the number of locations with the code value per lake\n",
    "# lakes_with = hasGpi[['water_name', 'location']].groupby(['water_name']).nunique('location')\n",
    "\n",
    "# total gpi found and percent of total\n",
    "# numg112 = dfS[dfS.code.isin(group_one)].quantity.sum()\n",
    "# numtotal = dfS.quantity.sum()\n",
    "# g112_p_total = numg112/numtotal\n",
    "\n",
    "# Frequency, surveys with code \n",
    "# qGpiD = dfCode.groupby(['location', 'water_name','loc_date', 'date'], as_index=False).pcs_m.sum()\n",
    "# number_trys = len(qGpiD)\n",
    "# number_fail = sum(qGpiD.pcs_m == 0)\n",
    "# number_succ = len(qGpiD[qGpiD.pcs_m > 0])\n",
    "# print(\"CODE total values\")\n",
    "# print(numg112)\n",
    "# print(numtotal)\n",
    "# print(g112_p_total)\n",
    "\n",
    "# print(\"CODE trys and fails\")\n",
    "# print(number_trys)\n",
    "# print(number_fail)\n",
    "# print(number_succ)\n",
    "\n",
    "# correlation with geo variables\n",
    "# combining streets and buildings could be interesting:\n",
    "\n",
    "# g112Df['combined'] = g112Df.streets + g112Df.buildings\n",
    "\n",
    "# g112Df[['population', 'streets', 'buildings', 'rivs','combined', 'p_total', ]].corr('spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#008891\">Geographic scope of surveys</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import shapes\n",
    "gdf = gpd.read_file('resources/shapes/riparian_communities.shp')\n",
    "gdfch = gpd.read_file('resources/shapes/ch.shp')\n",
    "gdflakes = gpd.read_file('resources/shapes/project_lakes.shp')\n",
    "\n",
    "# make point layers, add survey results\n",
    "\n",
    "# get the beaches from the location column of the data\n",
    "beaches = dfBeaches.loc[dfBeaches.index.isin(groupdfs[\"dfDtAll\"].location.unique())].copy()\n",
    "\n",
    "# create map the median pcs/m to location\n",
    "median_map = groupdfs[\"dfDtAll\"].groupby('location').pcs_m.median().copy()\n",
    "\n",
    "# apply the map to the beach data:\n",
    "beaches['pcs_m'] = beaches.index.map(lambda x: median_map[x])\n",
    "\n",
    "# port to gpd and reproject to the correct crs:\n",
    "gbeaches = gpd.GeoDataFrame(beaches, crs='EPSG:4326', geometry=gpd.points_from_xy(beaches.longitude, beaches.latitude))\n",
    "gdbeaches = gbeaches.to_crs(gdf.crs)\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(12,12))\n",
    "axs.set_aspect('equal')\n",
    "\n",
    "# plot the shapes and points:\n",
    "gdfch.plot(ax=axs, zorder=0, color='white', edgecolor='black', linewidth=0.26)\n",
    "gdf.plot(ax=axs, zorder=1, color='white', edgecolor='black', label='riparian coumminities', linewidth=0.26)\n",
    "gdflakes.plot(ax=axs, zorder=2, color='dodgerblue', edgecolor='white', linewidth=0.26)\n",
    "gdbeaches.plot(ax=axs, zorder=3, color='red', edgecolor='black',marker='o', label='survey locataions',  markersize=80)\n",
    "\n",
    "# get and set the bounds\n",
    "minx, miny, maxx, maxy = gdfch.total_bounds\n",
    "axs.set_xticks([])\n",
    "axs.set_yticks([])\n",
    "axs.set_title(F\"Map {map_num}: Survey locations {end_date}.\", **title_k)\n",
    "axs.set_xlim(minx, maxx)\n",
    "axs.set_ylim(miny, maxy)\n",
    "map_num + 1\n",
    "\n",
    "handles, labels = axs.get_legend_handles_labels()\n",
    "axs.legend(handles, labels)\n",
    "plt.tight_layout()\n",
    "\n",
    "maponefile = F\"{project_directory}/mapone.jpg\"\n",
    "files_generated.append(maponefile)\n",
    "plt.savefig(maponefile, dpi=300)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#1e90ff\">Survey results</span>\n",
    "(table 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# make summary of data and put to table:\n",
    "# summarize the total pcs/m per survey\n",
    "# use pd.describe and omit the first element (count)\n",
    "sum_tables = []\n",
    "for name in dt_names:\n",
    "    a_row = list(groupdfs[name].pcs_m.describe().index[1:])\n",
    "    a_data = [[np.round(groupdfs[name].pcs_m.describe()[i], 2)] for i in a_row]\n",
    "    one_table = {'data':a_data, 'a_row':a_row, 'name':name}\n",
    "    sum_tables.append(one_table)\n",
    "sum_tables[2]['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(1,len(sum_tables), figsize=(8,8))\n",
    "\n",
    "# for i, table in enumerate(sum_tables):\n",
    "#     axs[i].spines[\"top\"].set_visible(False)\n",
    "#     axs[i].spines[\"right\"].set_visible(False)\n",
    "#     axs[i].spines[\"bottom\"].set_visible(False)\n",
    "#     axs[i].spines[\"left\"].set_visible(False)\n",
    "#     axs[i].grid(False)\n",
    "#     axs[i].add_table(\n",
    "#         mpl.table.table(\n",
    "#             cellText=table['data'],\n",
    "#             rowLabels=table['a_row'],\n",
    "#             colLabels=['Summary'],\n",
    "#             colColours=['antiquewhite' for i in np.arange(1)],\n",
    "#             rowColours=['antiquewhite' for i in np.arange(len(table['a_row']))],\n",
    "#             ax=axs[i],\n",
    "#             **tablecenter_k))\n",
    "#     axs[i].tick_params(**tabtickp_k)\n",
    "#     axs[i].set_title(F\"Table {table_num}: {table['name'][:-3]}\", **title_k)\n",
    "#     table_num += 1\n",
    "    \n",
    "\n",
    "# plt.suptitle(F\"Survey results, key values {end_date}, n={len(groupdfs['dfDtAll'])}\", x=.5, y=0.98, ha='center', fontsize=(14))\n",
    "# tableonefile = F\"{project_directory}/tableone.jpg\"\n",
    "# files_generated.append(tableonefile)\n",
    "# plt.savefig(tableonefile, dpi=300)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.box(on=None)\n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#008891\">Regional summary</span>\n",
    "\n",
    "(table 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# adjust table args if need:\n",
    "tablecenter_k = dict(loc=\"top left\", bbox=(0,0,1,1), cellLoc='center', fontsize=12)\n",
    "\n",
    "fig, axs = plt.subplots(len(regional_summaries)-1, 1, figsize=(14,16))\n",
    "\n",
    "for i, name in enumerate(dt_names[:-1]):\n",
    "    axs[i].add_table(mpl.table.table(cellText=regional_summaries[i]['data'] , colLabels=regional_summaries[i]['cols'],colColours=['antiquewhite' for col in regional_summaries[i]['cols']], ax=axs[i], **tablecenter_k))\n",
    "    axs[i].grid(False)\n",
    "    axs[i].spines[\"top\"].set_visible(False)\n",
    "    axs[i].spines[\"right\"].set_visible(False)\n",
    "    axs[i].spines[\"bottom\"].set_visible(False)\n",
    "    axs[i].spines[\"left\"].set_visible(False)\n",
    "    axs[i].tick_params(**tabtickp_k)\n",
    "    axs[i].set_title(F\"Table {table_num}: Regional survey results, {name[:-3]}\", **titler_k)\n",
    "    table_num += 1\n",
    "\n",
    "# tabletwofile = F\"{project_directory}/tabletwo.jpg\"\n",
    "# files_generated.append(tabletwofile)\n",
    "# plt.savefig(tabletwofile, dpi=300)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.box(on=None)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#008891\">Distribution of survey results</span>\n",
    "(fig 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "monthly_median = {}\n",
    "for name in dt_names:\n",
    "    somdata = groupdfs[name].copy()\n",
    "    somdata.set_index('date', inplace=True)\n",
    "    monthly = somdata.resample('M').pcs_m.median()\n",
    "    monthly_median.update({name:[monthly, somdata]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,1,figsize=(10,10))\n",
    "colors = ['dodgerblue', 'salmon', 'teal', 'black']\n",
    "\n",
    "# results by lake\n",
    "for i,name in enumerate(dt_names[:-1]):\n",
    "    data = monthly_median[name][1]\n",
    "    sns.scatterplot(data=data, x=data.index,  y='pcs_m', label=name[:-3], color=colors[:-1][i], ax=axs[0])\n",
    "for i,name in enumerate(dt_names[:-1]):\n",
    "    data = monthly_median[name][0]\n",
    "    sns.lineplot(data=data, x=data.index,  y=data, label=name[:-3], color=colors[i], ax=axs[1])\n",
    "axs[0].xaxis.set_minor_locator(days)\n",
    "axs[0].xaxis.set_major_formatter(mths_fmt)\n",
    "axs[0].xaxis.set_major_locator(months)\n",
    "axs[0].tick_params(which='major', pad=10)\n",
    "axs[0].set_xlabel(\"\")\n",
    "labels, handles = axs[0].get_legend_handles_labels()\n",
    "axs[0].legend(labels, handles)\n",
    "axs[0].set_title(F\"Figure {figure_num}:survey results: {start_date} - {end_date}\", **title_k)\n",
    "figure_num += 1\n",
    "    \n",
    "axs[1].set_title(F\"Figure {figure_num}:median monthly survey results: {start_date} - {end_date}\", **title_k)\n",
    "figure_num +=1\n",
    "axs[1].xaxis.set_minor_locator(days)\n",
    "axs[1].xaxis.set_major_formatter(mths_fmt)\n",
    "axs[1].xaxis.set_major_locator(months)\n",
    "axs[1].tick_params(which='major', pad=10)\n",
    "axs[1].set_xlabel(\"\")\n",
    "labels, handles = axs[1].get_legend_handles_labels()\n",
    "axs[1].legend(labels, handles)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,len(dt_names), figsize=(12,5))\n",
    "\n",
    "\n",
    "for i,name in enumerate(dt_names):\n",
    "    data = groupdfs[name]\n",
    "    sns.histplot(\n",
    "        data=data,\n",
    "        x=data.pcs_m,\n",
    "        label=name[:-3],\n",
    "        color=colors[i],\n",
    "        stat='count',\n",
    "        ax=axs[i])\n",
    "    axs[i].set_title(F\"Figure {figure_num}: {name[:-3]}\", **title_k)\n",
    "    figure_num += 1\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#008891\">Definition of significant values</span>\n",
    "\n",
    "Significant values are those survey values that are equal to or exceed the 90th percentile of all survey results for the defined code.\n",
    "\n",
    "(table 3, fig 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tablecenter_k = dict(loc=\"top left\", bbox=(0,0,1,1), cellLoc='left', fontsize=10, colWidths = [0.4, 0.3, 0.3])\n",
    "\n",
    "for i,name in enumerate(dt_names):\n",
    "    fig, axs = plt.subplots(1,2, figsize=(10,6))\n",
    "    \n",
    "    # table of significant events\n",
    "    axs[0].add_table(mpl.table.table(\n",
    "        cellText=events_data[name].values,\n",
    "        colLabels=events_data[name].columns,\n",
    "        colColours=['antiquewhite' for col in events_data[name].columns],\n",
    "        ax=axs[0],\n",
    "        **tablecenter_k))\n",
    "#     axs[0].set_fontsize(12)\n",
    "    axs[0].grid(False)\n",
    "    axs[0].spines[\"top\"].set_visible(False)\n",
    "    axs[0].spines[\"right\"].set_visible(False)\n",
    "    axs[0].spines[\"bottom\"].set_visible(False)\n",
    "    axs[0].spines[\"left\"].set_visible(False)\n",
    "    axs[0].tick_params(**tabtickp_k)\n",
    "    axs[0].set_title(F\"Table {table_num}: {name[:-3]} significant events per lake\", **titler_k)\n",
    "    table_num+=1\n",
    "    \n",
    "#     significant events by lake\n",
    "    ax2 = sns.scatterplot(data=groupdfs[name],\n",
    "                          x='water_name',\n",
    "                          y='pcs_m',\n",
    "                          hue='water_name',\n",
    "                          style='significant',\n",
    "                          s=120, ax=axs[1],\n",
    "                          palette='husl',\n",
    "                          markers={True:'X', False:'s'})\n",
    "    ax2.xaxis.set_tick_params(rotation=45)\n",
    "    for tick in ax.xaxis.get_majorticklabels():\n",
    "        tick.set_horizontalalignment(\"right\")\n",
    "    ax2.set_xlabel(\"\")\n",
    "    ax2.set_ylabel(\"pieces per meter\", **ylab_k)\n",
    "    handles, labels = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(handles[9:], labels[9:])\n",
    "    ax2.set_title(F\"Figure {figure_num}: {name[:-3]} significant events\", **titler_k)\n",
    "    figure_num += 1\n",
    "    \n",
    "    plt.tight_layout()\n",
    "#     figfourfile = F\"{project_directory}/figurefourtablethree.jpg\"\n",
    "#     files_generated.append(figfourfile)\n",
    "#     plt.savefig(figfourfile,dpi=300)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#008891\">Geographic scope</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# make the data for a map\n",
    "# map median pcs_m to location\n",
    "new_map_data = {}\n",
    "for i,name in enumerate(dt_names):\n",
    "    somdata = groupdfs[name]\n",
    "    \n",
    "    \n",
    "    median_map = somdata.groupby('location').pcs_m.median().copy()\n",
    "\n",
    "    # identify the beaches with no code:\n",
    "    nogpi = somdata[somdata.pcs_m == 0]\n",
    "    noGpibeaches = dfBeaches.loc[dfBeaches.index.isin(nogpi.location.unique())].copy()\n",
    "    # apply the map\n",
    "#     noGpibeaches['pcs_m'] = noGpibeaches.index.map(lambda x: median_map[x])\n",
    "\n",
    "    # repeat for the places where code was found:\n",
    "    hasGpi = somdata[somdata.pcs_m > 0]\n",
    "    gpbeaches = dfBeaches.loc[dfBeaches.index.isin(hasGpi.location.unique())].copy()\n",
    "    gpbeaches['pcs_m'] = gpbeaches.index.map(lambda x: median_map[x])\n",
    "\n",
    "    # repeat one more time for significant events:\n",
    "    sign = somdata[somdata.significant == True]\n",
    "    gpsbeaches = dfBeaches.loc[dfBeaches.index.isin(sign.location.unique())].copy()\n",
    "    gpsbeaches['pcs_m'] = gpsbeaches.index.map(lambda x: median_map[x])\n",
    "\n",
    "    # call gpd on datafarames\n",
    "    gbeaches = gpd.GeoDataFrame(noGpibeaches, crs='EPSG:4326', geometry=gpd.points_from_xy(noGpibeaches.longitude, noGpibeaches.latitude))\n",
    "    gpibeaches = gpd.GeoDataFrame(gpbeaches, crs='EPSG:4326', geometry=gpd.points_from_xy(gpbeaches.longitude, gpbeaches.latitude))\n",
    "    gpsigbeaches = gpd.GeoDataFrame(gpsbeaches, crs='EPSG:4326', geometry=gpd.points_from_xy(gpsbeaches.longitude, gpsbeaches.latitude))\n",
    "\n",
    "    # reproject to correct crs\n",
    "    gdbeaches = gbeaches.to_crs(gdf.crs)\n",
    "    gpibeaches = gpibeaches.to_crs(gdf.crs)\n",
    "    gpsigbeaches = gpsigbeaches.to_crs(gdf.crs)\n",
    "    ma_p_data = {name:[gdbeaches, gpibeaches, gpsigbeaches]}\n",
    "    \n",
    "    new_map_data.update(ma_p_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "colors = ['red', 'blue', 'green']\n",
    "markers = ['p', 'x', 'o']\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(12,12))\n",
    "axs.set_aspect('equal')\n",
    "\n",
    "# plot the geo data\n",
    "gdfch.plot(ax=axs, zorder=0, color='white', edgecolor='black', linewidth=0.26)\n",
    "gdf.plot(ax=axs, zorder=1, color='white', edgecolor='black', label='riparian coumminities', linewidth=0.26)\n",
    "gdflakes.plot(ax=axs, zorder=2, color='dodgerblue', edgecolor='white', linewidth=0.26)\n",
    "for j, name in enumerate(dt_names[:2]):\n",
    "    somdata = new_map_data[name][1]\n",
    "    somdata.plot(ax=axs, zorder=3, color=colors[j], edgecolor='white',marker=markers[j], alpha=0.8, label=F\"{name[:-3]}\", markersize=(140-(10*i)))\n",
    "#         .plot(ax=axs, zorder=3, color='yellow', edgecolor='red',marker='o', label=F\"{code} found\", markersize=80)\n",
    "# gpsigbeaches.plot(ax=axs, zorder=4, color='red', edgecolor='red',marker='o', label=F\"{code} significant\", markersize=80)\n",
    "\n",
    "# get and set map bounds\n",
    "minx, miny, maxx, maxy = gdfch.total_bounds\n",
    "axs.set_xticks([])\n",
    "axs.set_yticks([])\n",
    "axs.set_title(F\"Map 2: {group_one_name} location of significant events {end_date}.\", **title_k)\n",
    "axs.set_xlim(minx, maxx)\n",
    "axs.set_ylim(miny, maxy)\n",
    "\n",
    "handles, labels = axs.get_legend_handles_labels()\n",
    "axs.legend(handles, labels)\n",
    "\n",
    "plt.tight_layout()\n",
    "maptwofile = F\"{project_directory}/maptwo.jpg\"\n",
    "files_generated.append(maptwofile)\n",
    "plt.savefig(maptwofile, dpi=300)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# make summary of data and put to table:\n",
    "leman_data = {k:v.loc[v.water_name == lake] for k,v in groupdfs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#1e90ff\">Lac Léman results and conclusions</span>\n",
    "table 4, figure 5, table 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# make summary of data and put to table:\n",
    "leman_data = {k:v.loc[v.water_name == lake] for k,v in groupdfs.items()}\n",
    "# summarize the total pcs/m per survey:\n",
    "# use pd.describe and omit the first element (count):\n",
    "leman_summary = [[np.round(leman_data['less than 5mm_dt'].pcs_m.describe()[i], 2)] for i in leman_data['less than 5mm_dt'].pcs_m.describe().index[1:]]\n",
    "# use the index for the row labels:\n",
    "dtotal_rowlabels=leman_data['less than 5mm_dt'].pcs_m.describe().index[1:]\n",
    "\n",
    "# repeat for the code o\n",
    "g112Dt_summary = [[np.round(leman_data['waste water_dt'].pcs_m.describe()[i], 2)] for i in leman_data['waste water_dt'].pcs_m.describe().index[1:]]\n",
    "g112Dt_rowlabels=leman_data['waste water_dt'].pcs_m.describe().index[1:]\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(8,8))\n",
    "tablecenter_k = dict(loc=\"top left\", bbox=(0,0,1,1), cellLoc='left', fontsize=10, colWidths = [1])\n",
    "\n",
    "anax = axs[0].add_table(mpl.table.table(cellText=leman_summary,rowLabels=dtotal_rowlabels, colLabels=['Summary'],colColours=['antiquewhite' for i in np.arange(1)], rowColours=['antiquewhite' for i in np.arange(len(dtotal_rowlabels))],  ax=axs[0], **tablecenter_k))\n",
    "anax.set_fontsize(12)\n",
    "axs[0].grid(False)\n",
    "axs[0].spines[\"top\"].set_visible(False)\n",
    "axs[0].spines[\"right\"].set_visible(False)\n",
    "axs[0].spines[\"bottom\"].set_visible(False)\n",
    "axs[0].spines[\"left\"].set_visible(False)\n",
    "axs[0].tick_params(**tabtickp_k)\n",
    "axs[0].set_title(F\"Survey totals\", **titler_k)\n",
    "\n",
    "\n",
    "anax1 = axs[1].add_table(mpl.table.table(cellText=g112Dt_summary,rowLabels=g112Dt_rowlabels, colLabels=['Summary'],colColours=['antiquewhite' for i in np.arange(1)], rowColours=['antiquewhite' for i in np.arange(len(g112Dt_rowlabels))],  ax=axs[1], **tablecenter_k))\n",
    "anax.set_fontsize(12)\n",
    "axs[1].grid(False)\n",
    "axs[1].spines[\"top\"].set_visible(False)\n",
    "axs[1].spines[\"right\"].set_visible(False)\n",
    "axs[1].spines[\"bottom\"].set_visible(False)\n",
    "axs[1].spines[\"left\"].set_visible(False)\n",
    "axs[1].tick_params(**tabtickp_k)\n",
    "axs[1].set_title(F\"{code} survey totals\", **titler_k)\n",
    "\n",
    "\n",
    "plt.suptitle(F\"table 4: Key values {lake} survey results, n={len(leman_data['waste water_dt'])}\", x=.5, y=1, ha='center', fontsize=(14))\n",
    "plt.tight_layout()\n",
    "# tablefourfile = F\"{project_directory}/tablefour.jpg\"\n",
    "# files_generated.append(tablefourfile)\n",
    "# plt.savefig(tablefourfile, dpi=300)\n",
    "plt.box(on=None)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,2, figsize=(12,12))\n",
    "\n",
    "# results grouped by beach\n",
    "ax = sns.scatterplot(data=leman_data['less than 5mm_dt'], x='location',  y='pcs_m', style='significant', markers={True:'X', False:'s'}, hue='location', marker=\"s\", s=80, palette='husl', ax=axs[0,0])\n",
    "ax.xaxis.set_tick_params(rotation=45)\n",
    "for tick in ax.xaxis.get_majorticklabels():\n",
    "    tick.set_horizontalalignment(\"right\")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.get_legend().remove()\n",
    "ax.set_title(F\"Figure {figure_num} : {dt_names[0][:-3]} pcs/m per location\", **title_k)\n",
    "ax.set_ylabel(\"Pieces per meter\", **ylab_k)\n",
    "figure_num += 1\n",
    "\n",
    "# results by date\n",
    "ax2 = sns.scatterplot(data=leman_data['waste water_dt'], x='location',  y='pcs_m', style='significant', markers={True:'X', False:'s'}, hue='location', marker=\"s\", s=80, palette='husl', ax=axs[0,1])\n",
    "ax2.xaxis.set_tick_params(rotation=45)\n",
    "for tick in ax2.xaxis.get_majorticklabels():\n",
    "    tick.set_horizontalalignment(\"right\")\n",
    "ax2.set_xlabel(\"\")\n",
    "ax2.get_legend().remove()\n",
    "ax2.set_title(F\"Figure {figure_num} : {dt_names[1][:-3]} pcs/m per location\", **title_k)\n",
    "ax2.set_ylabel(\"Pieces per meter\", **ylab_k)\n",
    "figure_num += 1\n",
    "# ax2 = sns.scatterplot(data=leman_data['less than 5mm_dt'], x='date', y='pcs_m', hue='location', style='significant',s=80, palette='husl', markers={True:'X', False:'s'},ax=axs[0,1])\n",
    "# ax2.set_title(F\"{code} Lac Léman survey results\", **title_k)\n",
    "# ax2.xaxis.set_minor_locator(days)\n",
    "# ax2.xaxis.set_major_formatter(mths_fmt)\n",
    "# ax2.xaxis.set_major_locator(months)\n",
    "# ax2.tick_params(which='major', pad=10)\n",
    "# ax2.set_xlabel(\"\")\n",
    "# ax2.set_ylabel(\"Pieces per meter\", **ylab_k)\n",
    "\n",
    "\n",
    "# table of significant events\n",
    "ax3 = sns.ecdfplot(data=leman_data['less than 5mm_dt'], x='pcs_m',label='less than 5mm', ax=axs[1,0])\n",
    "ax3 = sns.ecdfplot(data=leman_data['waste water_dt'], x='pcs_m', label='waste water',  ax=axs[1,0])\n",
    "ax3.set_title(F\"Figure {figure_num} :ECDF pieces per meter\", **title_k)\n",
    "ax3.set_xlabel(\"pieces per meter\", **xlab_k)\n",
    "ax3.set_ylabel(\"Ratio of samples\", **ylab_k)\n",
    "labels, handles = ax3.get_legend_handles_labels()\n",
    "ax3.legend(labels, handles)\n",
    "figure_num += 1\n",
    "# to be decided\n",
    "ax4 = sns.ecdfplot(data=leman_data['less than 5mm_dt'], x='p_total',label='less than 5mm', ax=axs[1,1])\n",
    "ax4 = sns.ecdfplot(data=leman_data['waste water_dt'], x='p_total', label='waste water',  ax=axs[1,1])\n",
    "ax4.set_title(F\"Figure {figure_num} :ECDF percent of daily total\", **title_k)\n",
    "ax4.set_xlabel(\"percent of daily total\", **xlab_k)\n",
    "ax4.set_ylabel(\"Ratio of samples\", **ylab_k)\n",
    "labels, handles = ax4.get_legend_handles_labels()\n",
    "ax4.legend(labels, handles)\n",
    "figure_num += 1\n",
    "\n",
    "plt.suptitle(F\"Figure 5: Lac Léman Disrtibution of {code} pcs/m, n={len(leman_data['less than 5mm_dt'])}\", x=.98, y=1, ha='right', fontsize=(14))\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "# figfivefile = F\"{project_directory}/figurefive.jpg\"\n",
    "# files_generated.append(figfivefile)\n",
    "# plt.savefig(figfivefile, dpi=300)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#1e90ff\">Use and distribution</span>\n",
    "(map 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def splitText(string, w=3):\n",
    "    words = string.split()\n",
    "    g = [' '.join(words[i: i + w]) for i in range(0, len(words), w)]\n",
    "    a_stack =  '\\n'.join(g)\n",
    "    return a_stack\n",
    "dfCodes['stacked'] = dfCodes.description.map(lambda x:splitText(x, w=3))\n",
    "tablecenter_k = dict(loc=\"top left\", bbox=(0,0,1,1), cellLoc='left', fontsize=10, colWidths = [0.2, 0.6, 0.2])\n",
    "\n",
    "for i, group in enumerate(project_groups):\n",
    "    print(group)\n",
    "    height=(len(group)+1)*1.5\n",
    "    fig, axs = plt.subplots(figsize=(6, height))\n",
    "    som_data = dfCodes[dfCodes.code.isin(group)][['code', 'stacked', 'material']]\n",
    "    a = plt.table(cellText=som_data.values,\n",
    "                  colLabels=som_data.columns,\n",
    "                  colColours=['antiquewhite' for col in data.columns],\n",
    "                  **tablecenter_k)\n",
    "    axs.add_table(a)\n",
    "    axs.grid(False)\n",
    "    axs.spines[\"top\"].set_visible(False)\n",
    "    axs.spines[\"right\"].set_visible(False)\n",
    "    axs.spines[\"bottom\"].set_visible(False)\n",
    "    axs.spines[\"left\"].set_visible(False)\n",
    "    axs.tick_params(**tabtickp_k)\n",
    "    axs.set_title(F\"Table {table_num}: Regional survey results, {dt_names[i][:-3]}\", **titler_k)\n",
    "    table_num += 1\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# get the beaches for the lake in question\n",
    "lemanbeaches =  dfBeaches.loc[dfBeaches.water_name == lake].copy()\n",
    "\n",
    "# find the ones that have zero for the code quantity or pcs_m:\n",
    "noGpibeaches = lemanbeaches.loc[lemanbeaches.index.isin(noGpi.location.unique())].copy()\n",
    "\n",
    "# make a map of the location and the median pcs_m for that location\n",
    "median_map = g112Df.groupby('location').pcs_m.median().copy()\n",
    "\n",
    "# map location of no gpi to the median value in g112Df:\n",
    "noGpibeaches['pcs_m'] = noGpibeaches.index.map(lambda x: median_map[x])\n",
    "\n",
    "# get the ones that have greater than zero for the code:\n",
    "gpbeaches = lemanbeaches.loc[lemanbeaches.index.isin(hasGpi.location.unique())].copy()\n",
    "\n",
    "# map location of gpbeaches to the median value in g112Df:\n",
    "gpbeaches['pcs_m'] = gpbeaches.index.map(lambda x: median_map[x])\n",
    "\n",
    "gpsbeaches = lemanbeaches.loc[lemanbeaches.index.isin(p_dt90_s.location.unique())].copy()\n",
    "gpsbeaches['pcs_m'] = gpsbeaches.index.map(lambda x: median_map[x])\n",
    "\n",
    "# make geodata frames\n",
    "gbeaches = gpd.GeoDataFrame(noGpibeaches, crs='EPSG:4326', geometry=gpd.points_from_xy(noGpibeaches.longitude, noGpibeaches.latitude))\n",
    "gpibeaches = gpd.GeoDataFrame(gpbeaches, crs='EPSG:4326', geometry=gpd.points_from_xy(gpbeaches.longitude, gpbeaches.latitude))\n",
    "gpsigbeaches = gpd.GeoDataFrame(gpsbeaches, crs='EPSG:4326', geometry=gpd.points_from_xy(gpsbeaches.longitude, gpsbeaches.latitude))\n",
    "\n",
    "# get shape files \n",
    "leman_com = gpd.read_file('resources/shapes/leman_communesx.shp')\n",
    "leman_filled = gpd.read_file('resources/shapes/leman_filledx.shp')\n",
    "leman_ints = gpd.read_file('resources/shapes/leman_intersectsx.shp')\n",
    "\n",
    "# if you have distribution points or user points load them here:\n",
    "# gpi_sp = gpd.read_file('resources/shapes/gpi_spointsx.shp')\n",
    "\n",
    "gdbeaches = gbeaches.to_crs(leman_ints.crs)\n",
    "gpibeaches = gpibeaches.to_crs(leman_ints.crs)\n",
    "gpsigbeaches = gpsigbeaches.to_crs(leman_ints.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(figsize=(12,12))\n",
    "axs.set_aspect('equal')\n",
    "\n",
    "# plot the data frames\n",
    "leman_ints.plot(ax=axs, zorder=1, color='dodgerblue', linewidth=0.26)\n",
    "leman_filled.plot(ax=axs, zorder=2, color='dodgerblue', edgecolor='dodgerblue', label='riparian coumminities', linewidth=0.26)\n",
    "leman_com.plot(ax=axs, zorder=0,  color='white', edgecolor='black', linewidth=0.26)\n",
    "gdbeaches.plot(ax=axs, zorder=3, color='slategray', edgecolor='black',marker='o', label=F\"{code} not found\", markersize=markerSize)\n",
    "gpibeaches.plot(ax=axs, zorder=4, color='yellow', edgecolor='red',marker='o', label=F\"{code} found\", markersize=markerSize)\n",
    "gpsigbeaches.plot(ax=axs, zorder=5, color='red', edgecolor='red',marker='o', label=F\"{code} significant\", markersize=markerSize)\n",
    "\n",
    "# these are distribution or user points, they are on top:\n",
    "# gpi_sp.plot(ax=axs, zorder=6, color='red', edgecolor='red',marker='x', label=F\"{code} supplier/user\", markersize=markerSize) \n",
    "\n",
    "\n",
    "# set min max\n",
    "minx, miny, maxx, maxy = leman_ints.total_bounds\n",
    "axs.set_xticks([])\n",
    "axs.set_yticks([])\n",
    "axs.set_title(F\"map 3: {lake} survey locations and {code} suppliers/users {end_date}.\", **title_k)\n",
    "axs.set_xlim(minx, maxx)\n",
    "axs.set_ylim(miny, maxy)\n",
    "\n",
    "# do work on legend if need\n",
    "handles, labels = axs.get_legend_handles_labels()\n",
    "axs.legend(handles, labels)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "mapthreefile = F\"{project_directory}/mapthree.jpg\"\n",
    "files_generated.append(mapthreefile)\n",
    "plt.savefig(mapthreefile, dpi=300)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# display the beach, the lake and the name of each municipality\n",
    "# considered in this report\n",
    "\n",
    "data = beaches[['water_name', 'location', 'city']].sort_values(by='water_name')\n",
    "data.rename(columns={'water_name':'Lake', 'location':'Location','city':'Municipality'}, inplace=True)\n",
    "tablecenter_k = dict(loc=\"top left\", bbox=(0,0,1,1), cellLoc='center', fontsize=14)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,32))\n",
    "ax1= ax.add_table(mpl.table.table(cellText=data.values, colLabels=data.columns,colColours=['antiquewhite' for col in data.columns], ax=ax, **tablecenter_k))\n",
    "ax.grid(False)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.spines[\"bottom\"].set_visible(False)\n",
    "ax.spines[\"left\"].set_visible(False)\n",
    "ax.tick_params(**tabtickp_k)\n",
    "ax.set_title(F\"annex a: Municipalities considered in this report\", **titler_k)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hopefully that just worked for you\n",
    "\n",
    "if not contact analyst@hammerdirt.ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def css_styling():\n",
    "    styles = open(F\"{project_directory}/custom.css\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
